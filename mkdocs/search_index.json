{
    "docs": [
        {
            "location": "/", 
            "text": "Build, Manage, and Deploy Your Application\n#\n\n\nMeet Layer0\n#\n\n\nLayer0 is a framework that helps you deploy web applications to the cloud with minimal fuss. Using a simple command line interface (CLI), you can manage the entire life cycle of your application without having to focus on infrastructure.\n\n\nReady to learn more about Layer0? See our \nintroduction page\n to learn about some important concepts. When you're ready to get started, take a look at the \ninstallation page\n for information about setting up Layer0.\n\n\nDownload\n#\n\n\n\n\n\n\n\n\nDownload \nv0.8.3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMac OSX\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\n\n\nContact Us\n#\n\n\nThe best way to contact the team is by joining the \n#xfra Slack channel\n. If you do not already have a Slack account, you can create one using your imshealth.com email address.", 
            "title": "Home"
        }, 
        {
            "location": "/#build-manage-and-deploy-your-application", 
            "text": "", 
            "title": "Build, Manage, and Deploy Your Application"
        }, 
        {
            "location": "/#meet-layer0", 
            "text": "Layer0 is a framework that helps you deploy web applications to the cloud with minimal fuss. Using a simple command line interface (CLI), you can manage the entire life cycle of your application without having to focus on infrastructure.  Ready to learn more about Layer0? See our  introduction page  to learn about some important concepts. When you're ready to get started, take a look at the  installation page  for information about setting up Layer0.", 
            "title": "Meet Layer0"
        }, 
        {
            "location": "/#download", 
            "text": "Download  v0.8.3             Mac OSX  Linux  Windows", 
            "title": "Download"
        }, 
        {
            "location": "/#contact-us", 
            "text": "The best way to contact the team is by joining the  #xfra Slack channel . If you do not already have a Slack account, you can create one using your imshealth.com email address.", 
            "title": "Contact Us"
        }, 
        {
            "location": "/releases/", 
            "text": "Version\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\n\n\n\n\nv0.8.3\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.8.2\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.8.1\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.8.0\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.7.2\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.7.1\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.7.0\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.6.3\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.6.2\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.6.1\n\n\nDarwin\n\n\nLinux\n\n\nWindows\n\n\n\n\n\n\nv0.6.0\n\n\nDarwin\n\n\nLinux\n\n\nWindows", 
            "title": "Releases"
        }, 
        {
            "location": "/intro/", 
            "text": "Layer0 Introduction\n#\n\n\nIn recent years, the process of deploying applications has seen incredible innovation. However, this innovation has taken a somewhat simple task and made it into something quite \ncomplicated\n. Cloud providers, load balancing, virtual servers, IP subnets, and a continuing list of technological considerations are not only required to be understood, but their creation and management must be automated for a modern application to be successful at scale.\n\n\nThe burden of understanding a complicated and ever-growing infrastructure is a large aspect of what Layer0 is trying to fix. We've already done the leg work for huge swathes of your backend infrastructure, and we've made it easy to tear down and start over again, too. Meanwhile, you can develop locally using \nDocker\n and be assured that your application will properly translate to the cloud when you're ready to deploy.\n\n\nLayer0 requires a solid understanding of Docker to get the most out of it. We highly recommend starting with \nDocker's Understanding the Architecture\n to learn more about using Docker locally and in the cloud. We also recommend the \nTwelve-Factor App\n primer, which is a critical resource for understanding how to build a microservice.\n\n\n\n\nLayer0 Concepts\n#\n\n\nThe following concepts are core Layer0 abstractions for the technologies and features we use \nbehind the scenes\n. These terms will be used throughout our guides, so having a general understanding of them is helpful.\n\n\nCertificates\n#\n\n\nAn SSL certificate obtained from a valid \nCertificate Authority (CA)\n. You can use these certificates to secure your HTTPS services by applying them to your Layer0 load balancers.\n\n\nDeploys\n#\n\n\nA \nmulticontainer docker configuration\n. This configuration file details how to deploy your application. We have several \nsample applications\n available that show what these files look like --- they're called \nDockerrun.aws.json\n within each sample app.\n\n\nLoad Balancers\n#\n\n\nA powerful tool that gives you the basic building blocks for high-availability, scaling, and HTTPS. We currently use Amazon's \nElastic Load Balancing\n, and it pays to understand the basics of this service when working with Layer0.\n\n\nServices\n#\n\n\nYour running Layer0 application. We also use the term \nservice\n for tools such as Consul, Logstash, and Shinken because they are Layer0 applications that we've pre-built for you.\n\n\nEnvironments\n#\n\n\nA logical grouping of services. Typically, you would make a single environment for each tier of your application, such as \ndev\n, \nstaging\n, and \nprod\n.", 
            "title": "Introduction"
        }, 
        {
            "location": "/intro/#layer0-introduction", 
            "text": "In recent years, the process of deploying applications has seen incredible innovation. However, this innovation has taken a somewhat simple task and made it into something quite  complicated . Cloud providers, load balancing, virtual servers, IP subnets, and a continuing list of technological considerations are not only required to be understood, but their creation and management must be automated for a modern application to be successful at scale.  The burden of understanding a complicated and ever-growing infrastructure is a large aspect of what Layer0 is trying to fix. We've already done the leg work for huge swathes of your backend infrastructure, and we've made it easy to tear down and start over again, too. Meanwhile, you can develop locally using  Docker  and be assured that your application will properly translate to the cloud when you're ready to deploy.  Layer0 requires a solid understanding of Docker to get the most out of it. We highly recommend starting with  Docker's Understanding the Architecture  to learn more about using Docker locally and in the cloud. We also recommend the  Twelve-Factor App  primer, which is a critical resource for understanding how to build a microservice.", 
            "title": "Layer0 Introduction"
        }, 
        {
            "location": "/intro/#layer0-concepts", 
            "text": "The following concepts are core Layer0 abstractions for the technologies and features we use  behind the scenes . These terms will be used throughout our guides, so having a general understanding of them is helpful.", 
            "title": "Layer0 Concepts"
        }, 
        {
            "location": "/intro/#certificates", 
            "text": "An SSL certificate obtained from a valid  Certificate Authority (CA) . You can use these certificates to secure your HTTPS services by applying them to your Layer0 load balancers.", 
            "title": "Certificates"
        }, 
        {
            "location": "/intro/#deploys", 
            "text": "A  multicontainer docker configuration . This configuration file details how to deploy your application. We have several  sample applications  available that show what these files look like --- they're called  Dockerrun.aws.json  within each sample app.", 
            "title": "Deploys"
        }, 
        {
            "location": "/intro/#load-balancers", 
            "text": "A powerful tool that gives you the basic building blocks for high-availability, scaling, and HTTPS. We currently use Amazon's  Elastic Load Balancing , and it pays to understand the basics of this service when working with Layer0.", 
            "title": "Load Balancers"
        }, 
        {
            "location": "/intro/#services", 
            "text": "Your running Layer0 application. We also use the term  service  for tools such as Consul, Logstash, and Shinken because they are Layer0 applications that we've pre-built for you.", 
            "title": "Services"
        }, 
        {
            "location": "/intro/#environments", 
            "text": "A logical grouping of services. Typically, you would make a single environment for each tier of your application, such as  dev ,  staging , and  prod .", 
            "title": "Environments"
        }, 
        {
            "location": "/setup/install/", 
            "text": "Install and Configure Layer0\n#\n\n\nPrerequisites\n#\n\n\nBefore you can install and configure Layer0, you must obtain the following:\n\n\n\n\n\n\nA \"Bring Your Own Operations\" (\nBYOO\n) account.\n\n\nIf you do not already have a BYOO account, submit an \ninstance creation request form\n.\nIf you are not able to access the form, send an email to \nxfra@us.imshealth.com\n. In your email, include your team name and the name of the team owner.\n\n\n\n\n\n\nA d.ims.io token.\n\n\nThis token grants you read access to the IMS private Docker Registry. You can use the same token for multiple Layer0 installations.\nTo generate a token, go to \nhttps://d.ims.io/token\n. Log in using your IMS Health domain credentials.\n\n\n\n\n\n\nAn EC2 Key Pair.\n This allows SSH access into the EC2 instances running your Services. \nYou can use an existing key pair if you've already made one.\nOtherwise, create an EC2 key pair in AWS following their \ninstructions\n.\nChoose any name for the key pair and make note of it. \n\n\n\n\n\n\nPart 1: Download and extract Layer0\n#\n\n\n\n\nIn the \nDownloads section of the home page\n, select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer.\n\n\nAdd both the directory that contains the \nl0\n application, as well as the entire l0-setup directory, to your system path. The l0-setup directory contains files that are necessary for the \nl0-setup\n application to work properly, so you must add the entire directory to your path. \nFor more information about adding directories to your system path, see the following resources:\n\n\n(Windows) \nHow to Edit Your System PATH for Easy Command Line Access in Windows\n\n\n(Linux/Mac) \nAdding a Directory to the Path\n\n\n\n\n\n\n\n\nPart 2: Create an Access Key\n#\n\n\nThis step will create an Identity \n Access Management (IAM) access key from your BYOO instance. You will use the credentials created in this section when installing, updating, or removing Layer0 resources. \n\n\nTo create an Access Key:\n\n\n\n\nIn a web browser, go to your team's BYOO login page. Log in using your IMS Health domain credentials. \n\n\nUnder \nSecurity and Identity\n, click \nIdentity and Access Management\n.\n\n\nClick \nGroups\n.\n\n\nClick \nCreate New Group\n and enter the name: \nAdministrators\n. Click \nNext Step\n.\n\n\nClick on \nAdministratorAccess\n to attach the Administator policy to your new group. Click \nNext Step\n to review and \nCreate Group\n to create your group.\n\n\nAfter creating your group, click on \nUsers\n.\n\n\nClick \nCreate New Users\n and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Ensure the box \nGenerate an Access Key for each user\n is checked, and click \nCreate\n.\n\n\nAfter your user is created successfully, you'll be presented with new user security credentials. Click \nDownload Credentials\n, which will save your new access key to a CSV file. Once your credentials have been downloaded, click \nClose\n.\n\n\nClick \nCreate New Users\n and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Ensure the box \nGenerate an Access Key for each user\n is checked, and click \nCreate\n.\n\n\nFind your newly created user in the Users list, and select it. Under \nUser Actions\n, select \nAdd User to Groups\n.\n\n\nSelect the group \nAdministrators\n and click \nAdd to Groups\n. This will make your newly created user an administrator for your BYOO account, so be sure to keep your security credentials safe!\n\n\n\n\nPart 3: Configure your Layer0\n#\n\n\nNow that you have downloaded Layer0 and configured your AWS instance, you can create your Layer0.\n\n\nTo configure Layer0:\n\n\n\n\nAt a command prompt, navigate to the \nl0-setup\n subdirectory in the folder in which you extracted the Layer0 files.\n\n\nType the following command, replacing \nprefix\n with a unique name for your Layer0:\n\n\n  \nl0-setup apply\n \nprefix\n\n\n\n\nWhen prompted, enter the following information:\n\n\nAWS Access Key ID\n: The access key ID contained in the credential file that you downloaded in step 5 of the previous section.\n\n\nAWS Secret Access Key\n: The secret access key contained in the credential file that you downloaded in step 5 of the previous section.\n\n\nd.ims.io Token\n: The Docker token that you created in the Prerequisites section.\n\n\nKey Pair\n: The name of the key pair that you created in the Prerequisites section.\n\n\n\n\n\n\n\n\nThe first time you run the \napply\n command, it may take around 15 minutes to complete. If the \napply\n command fails to complete successfully, it is safe to run it again until it succeeds.\n\n\nPart 4: Configure environment variables\n#\n\n\nOnce the \napply\n command has run successfully, you can configure the Layer0 environment variables using the \nendpoint\n command.\n\n\nTo view the environment variables for your Layer0, type the following command, replacing \nprefix\n with the Layer0 prefix you created in Part 2: \n\n\n  \nl0-setup endpoint --insecure\n \nprefix\n\n\n\n\nAlternatively, you can view the environment variables and apply them to your shell using a single command:\n\n\n\n\n\n\n\n\n\n\n(Windows PowerShell): \nl0-setup endpoint --insecure --powershell\n \nprefix\n \n| Out-String | Invoke-Expression\n\n\n(Linux/Mac): \neval \"$(l0-setup endpoint --insecure\n \nprefix\n)\"\n\n\n\n\nPart 5: Configure DNS Endpoint\n#\n\n\n\n\nNote\n\n\nThe procedures in this section are optional, but are highly recommended for production use.\n\n\n\n\nLayer0 is configured to use SSL with the imshealthlabs.net domain. Follow the procedures in this section to associate a subdomain of imshealthlabs.net with your Layer0 endpoint.\n\n\nTo associate a subdomain with your Layer0 endpoint:\n\n\n\n\nContact the \nDevOps team\n and request a user account on the DevOps AWS account.\n\n\nOnce you have received the DevOps AWS account credentials, sign in to the \nAWS Console\n.\n\n\nUnder \nNetworking\n, click \nRoute 53\n.\n\n\nIn the menu on the left side of the screen, click \nHosted zones\n.\n\n\nIn the list of domains, click \nimshealthlabs.net\n.\n\n\nClick \nCreate Record Set\n.\n\n\nIn the \nCreate Record Set\n panel on the right side of the screen, configure the record set:\n\n\nIn the \nName\n field, type the subdomain. The subdomain name you specify must be unique.\n\n\nFrom the \nType\n menu, select \nCNAME - Canonical name\n.\n\n\nIn the \nValue\n field, enter the endpoint URL for your Layer0. You can find the endpoint for your Layer0 by using the \nendpoint\n command; see part 4, above, for more information.\n\n\nClick \nCreate\n.", 
            "title": "Install"
        }, 
        {
            "location": "/setup/install/#install-and-configure-layer0", 
            "text": "", 
            "title": "Install and Configure Layer0"
        }, 
        {
            "location": "/setup/install/#prerequisites", 
            "text": "Before you can install and configure Layer0, you must obtain the following:    A \"Bring Your Own Operations\" ( BYOO ) account.  If you do not already have a BYOO account, submit an  instance creation request form .\nIf you are not able to access the form, send an email to  xfra@us.imshealth.com . In your email, include your team name and the name of the team owner.    A d.ims.io token.  This token grants you read access to the IMS private Docker Registry. You can use the same token for multiple Layer0 installations.\nTo generate a token, go to  https://d.ims.io/token . Log in using your IMS Health domain credentials.    An EC2 Key Pair.  This allows SSH access into the EC2 instances running your Services. \nYou can use an existing key pair if you've already made one.\nOtherwise, create an EC2 key pair in AWS following their  instructions .\nChoose any name for the key pair and make note of it.", 
            "title": "Prerequisites"
        }, 
        {
            "location": "/setup/install/#part-1-download-and-extract-layer0", 
            "text": "In the  Downloads section of the home page , select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer.  Add both the directory that contains the  l0  application, as well as the entire l0-setup directory, to your system path. The l0-setup directory contains files that are necessary for the  l0-setup  application to work properly, so you must add the entire directory to your path.  For more information about adding directories to your system path, see the following resources:  (Windows)  How to Edit Your System PATH for Easy Command Line Access in Windows  (Linux/Mac)  Adding a Directory to the Path", 
            "title": "Part 1: Download and extract Layer0"
        }, 
        {
            "location": "/setup/install/#part-2-create-an-access-key", 
            "text": "This step will create an Identity   Access Management (IAM) access key from your BYOO instance. You will use the credentials created in this section when installing, updating, or removing Layer0 resources.   To create an Access Key:   In a web browser, go to your team's BYOO login page. Log in using your IMS Health domain credentials.   Under  Security and Identity , click  Identity and Access Management .  Click  Groups .  Click  Create New Group  and enter the name:  Administrators . Click  Next Step .  Click on  AdministratorAccess  to attach the Administator policy to your new group. Click  Next Step  to review and  Create Group  to create your group.  After creating your group, click on  Users .  Click  Create New Users  and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Ensure the box  Generate an Access Key for each user  is checked, and click  Create .  After your user is created successfully, you'll be presented with new user security credentials. Click  Download Credentials , which will save your new access key to a CSV file. Once your credentials have been downloaded, click  Close .  Click  Create New Users  and enter a unique user name you will use for Layer0. This user name can be used for multiple Layer0 installations. Ensure the box  Generate an Access Key for each user  is checked, and click  Create .  Find your newly created user in the Users list, and select it. Under  User Actions , select  Add User to Groups .  Select the group  Administrators  and click  Add to Groups . This will make your newly created user an administrator for your BYOO account, so be sure to keep your security credentials safe!", 
            "title": "Part 2: Create an Access Key"
        }, 
        {
            "location": "/setup/install/#part-3-configure-your-layer0", 
            "text": "Now that you have downloaded Layer0 and configured your AWS instance, you can create your Layer0.  To configure Layer0:   At a command prompt, navigate to the  l0-setup  subdirectory in the folder in which you extracted the Layer0 files.  Type the following command, replacing  prefix  with a unique name for your Layer0: \n   l0-setup apply   prefix   When prompted, enter the following information:  AWS Access Key ID : The access key ID contained in the credential file that you downloaded in step 5 of the previous section.  AWS Secret Access Key : The secret access key contained in the credential file that you downloaded in step 5 of the previous section.  d.ims.io Token : The Docker token that you created in the Prerequisites section.  Key Pair : The name of the key pair that you created in the Prerequisites section.     The first time you run the  apply  command, it may take around 15 minutes to complete. If the  apply  command fails to complete successfully, it is safe to run it again until it succeeds.", 
            "title": "Part 3: Configure your Layer0"
        }, 
        {
            "location": "/setup/install/#part-4-configure-environment-variables", 
            "text": "Once the  apply  command has run successfully, you can configure the Layer0 environment variables using the  endpoint  command.  To view the environment variables for your Layer0, type the following command, replacing  prefix  with the Layer0 prefix you created in Part 2:  \n   l0-setup endpoint --insecure   prefix   Alternatively, you can view the environment variables and apply them to your shell using a single command:     (Windows PowerShell):  l0-setup endpoint --insecure --powershell   prefix   | Out-String | Invoke-Expression  (Linux/Mac):  eval \"$(l0-setup endpoint --insecure   prefix )\"", 
            "title": "Part 4: Configure environment variables"
        }, 
        {
            "location": "/setup/install/#part-5-configure-dns-endpoint", 
            "text": "Note  The procedures in this section are optional, but are highly recommended for production use.   Layer0 is configured to use SSL with the imshealthlabs.net domain. Follow the procedures in this section to associate a subdomain of imshealthlabs.net with your Layer0 endpoint.  To associate a subdomain with your Layer0 endpoint:   Contact the  DevOps team  and request a user account on the DevOps AWS account.  Once you have received the DevOps AWS account credentials, sign in to the  AWS Console .  Under  Networking , click  Route 53 .  In the menu on the left side of the screen, click  Hosted zones .  In the list of domains, click  imshealthlabs.net .  Click  Create Record Set .  In the  Create Record Set  panel on the right side of the screen, configure the record set:  In the  Name  field, type the subdomain. The subdomain name you specify must be unique.  From the  Type  menu, select  CNAME - Canonical name .  In the  Value  field, enter the endpoint URL for your Layer0. You can find the endpoint for your Layer0 by using the  endpoint  command; see part 4, above, for more information.  Click  Create .", 
            "title": "Part 5: Configure DNS Endpoint"
        }, 
        {
            "location": "/setup/update/", 
            "text": "Upgrade Layer0\n#\n\n\nThis section provides procedures for upgrading your Layer0 installation to the latest version.\n\n\nThe process of upgrading Layer0 differs depending on the version of Layer0 you are currently running, and the version you are upgrading to. Before continuing, determine which of the following sections applies to your current Layer0 installation:\n\n\n\n\nUpgrading from version 0.5.5 or earlier to the latest version of Layer0\n\n\nUpgrading from version 0.6.0 or 0.6.1 to the latest version of Layer0 0.6.2\n\n\nUpgrading ffrom version 0.7.x to 0.7.2\n\n\nUpgrading versions not listed above\n\n\n\n\n\n\nUpgrading from version 0.5.5 or earlier to the latest version of Layer0\n#\n\n\nThe architecture of Layer0 version 0.5.5 and earlier is not compatible with the architecture of more recent versions. For this reason, Layer0 versions 0.5.5 and earlier cannot be upgraded.\n\n\nIf you are using Layer0 0.5.5 or earlier and want to upgrade to a more recent version, we recommend that you create a new Layer0 using the current version of Layer0, and then migrate your services onto it by deploying your Docker task definitions into the new instance.\n\n\nIf you have questions about migrating from an older version of Layer0, visit the \n#xfra Slack channel\n.\n\n\n\n\nUpgrading from version 0.6.0 or 0.6.1 to the latest version of Layer0\n#\n\n\nIn order to upgrade from version 0.6.0 or 0.6.1 to the current version of Layer0, you must first complete some steps that are specific to these versions of Layer0.\n\n\nThis upgrade will move your Layer0 API server from Elastic Beanstalk to the EC2 Container Server (ECS), which will change the URL of your Layer0 API endpoint. After completing the procedures in this section, you will need to update any references to the API endpoint in your projects.\n\n\nTo update to the latest version:\n\n\n\n\nNote\n\n\nDo not delete the \nl0\n or \nl0-setup\n files from version 0.6.0 or 0.6.1 until step 3\nyou will need these files to complete this upgrade process.\n\n\n\n\n\n\nIn the \nDownloads section of the home page\n, select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer, but do not remove or replace the files from version 0.6.0 or 0.6.1.\n\n\nType the following command to destroy the ElasticBeanstalk resources, replacing \nprefix\n with the name of your Layer0 prefix:\n\n\n  \nl0-setup terraform\n \nprefix\n \ndestroy -target aws_elastic_beanstalk_application.api\n\n\n\n\nType the following command to backup the state files for your Layer0:\n\n\n  \nl0-setup backup\n \nprefix\n\n\n\n\nFrom the \nDownloads section of the home page\n, select the appropriate installation file for your operating system. Extract the resulting zip file to your computer, and then replace the \nl0\n and \nl0-setup\n files that are already in your system path with the versions that you just downloaded.\n\n\n\n\nType the following command to determine which version of the \nl0-setup\n application you are running:\n\n\n  \nl0-setup --version\n\n\n\nThe output of this command should equal the version number of the current version of Layer0.\n\nIf you see the correct version number, proceed to the next step. Otherwise, ensure that you have replaced the \nl0\n and \nl0-setup\n files in your system path with the new versions of these files.\n\n\n\n\n\n\nType the following command to restore the state files:\n\n\n  \nl0-setup restore\n \nprefix\n\n\n\n\n\n\nType the following command to update the API Docker Image tag to the appropriate version:\n\n\n  \nl0-setup plan\n \nprefix\n \n-var api_docker_image_tag=\nvX.Y.Z\n\n  \nReplace \nvX.Y.Z\n in the command above with the version number of the current version of Layer0.\n\n\n\n\n\n\nType the following command to apply the current version of Layer0:\n\n\n  \nl0-setup apply\n \nprefix\n\n\n\n\n\n\n\n\nAt this point, the environment variables already applied to your shell will no longer be valid. Type the following command to view the new environment variables and apply them to your shell:\n\n\n\n\n(Windows PowerShell): \nl0-setup endpoint --insecure --powershell\n \nprefix\n \n| Out-String | Invoke-Expression\n\n\n(Linux/Mac): \neval \"$(l0-setup endpoint --insecure\n \nprefix\n)\"\n\n\n\n\n\n\n\n\nVersions supported by these procedures\n#\n\n\nThe procedures above are known to work when migrating between the versions listed in the following table:\n\n\n\n\n\n\n\n\nExisting version\n\n\nNew version\n\n\n\n\n\n\n\n\n\n\n0.6.0\n\n\n0.6.2\n\n\n\n\n\n\n0.6.0\n\n\n0.6.3\n\n\n\n\n\n\n0.6.1\n\n\n0.6.2\n\n\n\n\n\n\n0.6.1\n\n\n0.6.3\n\n\n\n\n\n\n\n\n\n\nUpgrading version 0.7.x to 0.7.2\n#\n\n\nThe commands \nservice logs\n and \ntask logs\n will not work for Services and Tasks created prior to version 0.7.2. \nYou will likely get an error: \nAWS Error: the specified log group does not exist\n. \nYou must re-create your Service in order for Layer0 to create the proper log group. \n\n\n\n\nUpgrading versions not listed above\n#\n\n\n\n\nNote\n\n\nIf you have already completed the upgrade procedures listed in one of the sections above, you do not need to complete the procedures in this section.\n\n\n\n\nTo upgrade to a new version of Layer0:\n\n\n\n\nIn the \nDownloads section of the home page\n, select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer, and then move the \nl0\n and \nl0-setup\n files to a folder in your system path, replacing any previous versions of these files.\n\n\nFollow the instructions for creating an \nAdministrator Access Key\n. You will use this access key when l0-setup prompts you for AWS credentials.\n\n\nType the following command to verify that you are working with the correct version of Layer0:\n\n\n  \nl0-setup --version\n\n\n\nThe output of this command should display the version number of the most recent version of Layer0. If it does, proceed to the next step; if not, ensure that you copied the latest versions of \nl0\n and \nl0-setup\n to the appropriate directories in your system path.\n\n\n\n\nType the following command to restore the state files for your Layer0, replacing \nprefix\n with the name of your Layer0 prefix:\n\n\n  \nl0-setup restore\n \nprefix\n\n\n\n\n\n\n\n\nType the following command to update your api image tag:\n\n\n  \nl0-setup plan\n \nprefix\n \n-var api_docker_image_tag\n=\nversion\n \n\n\n\n\n\n\n\n\n(\nThis setup is only required when upgrading to v0.7.0 and higher\n) Type the following command to update your runner image tag:\n\n\n  \nl0-setup plan\n \nprefix\n \n-var runner_version_tag\n=\nversion\n \n\n\n\n\n\n\n\n\nType the following command to update your AWS Access Key ID from step 2:\n\n\n  \nl0-setup plan\n \nprefix\n \n-var api_access_key\n=\naccess_key_id\n \n\n\n\n\n\n\n\n\nType the following command to update your AWS Secret Access Key from step 2:\n\n\n  \nl0-setup plan\n \nprefix\n \n-var api_secret_key\n=\nsecret_key\n \n\n\n\n\n\n\n\n\nType the following command to apply the upgrade:\n\n\n  \nl0-setup apply\n \nprefix\n\n\n\n\n\n\n\n\nVersions supported by these procedures\n#\n\n\nThe procedures above are known to work when migrating between the versions listed in the following table:\n\n\n\n\n\n\n\n\nExisting version\n\n\n\n\nNew version\n\n\n\n\n\n\n\n\n\n\n0.6.2\n\n\n\n\n 0.6.2\n\n\n\n\n\n\n0.6.3\n\n\n\n\n 0.6.3\n\n\n\n\n\n\n0.7.0\n\n\n\n\n0.7.1", 
            "title": "Upgrade"
        }, 
        {
            "location": "/setup/update/#upgrade-layer0", 
            "text": "This section provides procedures for upgrading your Layer0 installation to the latest version.  The process of upgrading Layer0 differs depending on the version of Layer0 you are currently running, and the version you are upgrading to. Before continuing, determine which of the following sections applies to your current Layer0 installation:   Upgrading from version 0.5.5 or earlier to the latest version of Layer0  Upgrading from version 0.6.0 or 0.6.1 to the latest version of Layer0 0.6.2  Upgrading ffrom version 0.7.x to 0.7.2  Upgrading versions not listed above", 
            "title": "Upgrade Layer0"
        }, 
        {
            "location": "/setup/update/#upgrading-from-version-055-or-earlier-to-the-latest-version-of-layer0", 
            "text": "The architecture of Layer0 version 0.5.5 and earlier is not compatible with the architecture of more recent versions. For this reason, Layer0 versions 0.5.5 and earlier cannot be upgraded.  If you are using Layer0 0.5.5 or earlier and want to upgrade to a more recent version, we recommend that you create a new Layer0 using the current version of Layer0, and then migrate your services onto it by deploying your Docker task definitions into the new instance.  If you have questions about migrating from an older version of Layer0, visit the  #xfra Slack channel .", 
            "title": "Upgrading from version 0.5.5 or earlier to the latest version of Layer0"
        }, 
        {
            "location": "/setup/update/#upgrading-from-version-060-or-061-to-the-latest-version-of-layer0", 
            "text": "In order to upgrade from version 0.6.0 or 0.6.1 to the current version of Layer0, you must first complete some steps that are specific to these versions of Layer0.  This upgrade will move your Layer0 API server from Elastic Beanstalk to the EC2 Container Server (ECS), which will change the URL of your Layer0 API endpoint. After completing the procedures in this section, you will need to update any references to the API endpoint in your projects.  To update to the latest version:   Note  Do not delete the  l0  or  l0-setup  files from version 0.6.0 or 0.6.1 until step 3 you will need these files to complete this upgrade process.    In the  Downloads section of the home page , select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer, but do not remove or replace the files from version 0.6.0 or 0.6.1.  Type the following command to destroy the ElasticBeanstalk resources, replacing  prefix  with the name of your Layer0 prefix: \n   l0-setup terraform   prefix   destroy -target aws_elastic_beanstalk_application.api   Type the following command to backup the state files for your Layer0: \n   l0-setup backup   prefix   From the  Downloads section of the home page , select the appropriate installation file for your operating system. Extract the resulting zip file to your computer, and then replace the  l0  and  l0-setup  files that are already in your system path with the versions that you just downloaded.   Type the following command to determine which version of the  l0-setup  application you are running: \n   l0-setup --version  \nThe output of this command should equal the version number of the current version of Layer0. \nIf you see the correct version number, proceed to the next step. Otherwise, ensure that you have replaced the  l0  and  l0-setup  files in your system path with the new versions of these files.    Type the following command to restore the state files: \n   l0-setup restore   prefix    Type the following command to update the API Docker Image tag to the appropriate version: \n   l0-setup plan   prefix   -var api_docker_image_tag= vX.Y.Z \n   Replace  vX.Y.Z  in the command above with the version number of the current version of Layer0.    Type the following command to apply the current version of Layer0: \n   l0-setup apply   prefix     At this point, the environment variables already applied to your shell will no longer be valid. Type the following command to view the new environment variables and apply them to your shell:   (Windows PowerShell):  l0-setup endpoint --insecure --powershell   prefix   | Out-String | Invoke-Expression  (Linux/Mac):  eval \"$(l0-setup endpoint --insecure   prefix )\"", 
            "title": "Upgrading from version 0.6.0 or 0.6.1 to the latest version of Layer0"
        }, 
        {
            "location": "/setup/update/#versions-supported-by-these-procedures", 
            "text": "The procedures above are known to work when migrating between the versions listed in the following table:     Existing version  New version      0.6.0  0.6.2    0.6.0  0.6.3    0.6.1  0.6.2    0.6.1  0.6.3", 
            "title": "Versions supported by these procedures"
        }, 
        {
            "location": "/setup/update/#upgrading-version-07x-to-072", 
            "text": "The commands  service logs  and  task logs  will not work for Services and Tasks created prior to version 0.7.2. \nYou will likely get an error:  AWS Error: the specified log group does not exist . \nYou must re-create your Service in order for Layer0 to create the proper log group.", 
            "title": "Upgrading version 0.7.x to 0.7.2"
        }, 
        {
            "location": "/setup/update/#upgrading-versions-not-listed-above", 
            "text": "Note  If you have already completed the upgrade procedures listed in one of the sections above, you do not need to complete the procedures in this section.   To upgrade to a new version of Layer0:   In the  Downloads section of the home page , select the appropriate installation file for your operating system. Extract the zip file to a directory on your computer, and then move the  l0  and  l0-setup  files to a folder in your system path, replacing any previous versions of these files.  Follow the instructions for creating an  Administrator Access Key . You will use this access key when l0-setup prompts you for AWS credentials.  Type the following command to verify that you are working with the correct version of Layer0: \n   l0-setup --version  \nThe output of this command should display the version number of the most recent version of Layer0. If it does, proceed to the next step; if not, ensure that you copied the latest versions of  l0  and  l0-setup  to the appropriate directories in your system path.   Type the following command to restore the state files for your Layer0, replacing  prefix  with the name of your Layer0 prefix: \n   l0-setup restore   prefix     Type the following command to update your api image tag: \n   l0-setup plan   prefix   -var api_docker_image_tag = version       ( This setup is only required when upgrading to v0.7.0 and higher ) Type the following command to update your runner image tag: \n   l0-setup plan   prefix   -var runner_version_tag = version       Type the following command to update your AWS Access Key ID from step 2: \n   l0-setup plan   prefix   -var api_access_key = access_key_id       Type the following command to update your AWS Secret Access Key from step 2: \n   l0-setup plan   prefix   -var api_secret_key = secret_key       Type the following command to apply the upgrade: \n   l0-setup apply   prefix", 
            "title": "Upgrading versions not listed above"
        }, 
        {
            "location": "/setup/update/#versions-supported-by-these-procedures_1", 
            "text": "The procedures above are known to work when migrating between the versions listed in the following table:     Existing version   New version      0.6.2    0.6.2    0.6.3    0.6.3    0.7.0   0.7.1", 
            "title": "Versions supported by these procedures"
        }, 
        {
            "location": "/setup/destroy/", 
            "text": "Destroying a Layer0 instance\n#\n\n\nDuring testing or migration, you may find that you need to delete a non-functional or outdated instance of Layer0. This section provides procedures for destroying (deleting) a Layer0 instance.\n\n\nPart 1: Clean Up Your Layer0 Environments\n#\n\n\nIn order to destroy a Layer0 instance, you must first delete all environments in the instance.\n\n\nTo delete Layer0 environments:\n\n\n\n  \nAt the command prompt, type the following command to see a list of environments in your Layer0 instance:\n    \n\n      \nl0 environment list\n\n    \n\n\nFor each environment listed in the previous step, with the exception of the environments that begin with \"api\", issue the following command (where \nenvironmentName\n is the name of the environment you want to delete):\n    \n\n      \nl0 environment delete\n \nenvironmentName\n \n--wait\n\n    \n\n  Repeat this step until all of the environments, with the exception of the \"api\" environments, have been deleted.\n\n\nAt the command prompt, type the following command to list all of the certificates that exist in your Layer0 instance:\n    \n\n      \nl0 certificate list\n\n    \n\n  \n\n  \nFor each certificate listed in the previous step, with the exception of the certificate named \"imshealthlabs\", type the following command to delete the certificate (replacing \ncertificateName\n with the name of the certificate you want to delete):\n    \n\n      \nl0 certificate delete\n \ncertificateName\n \n--wait\n\n    \n\n    Repeat this step until all of the certificates, with the exception of the \"imshealthlabs\" certificate, have been deleted. When you have finished deleting the environments and certificates in your Layer0 instance, proceed to Part 2.\n  \n\n\n\n\nPart 2: Destroy the Layer0 instance\n#\n\n\nOnce you have prepared your Layer0 instance for deletion, you can use the \nl0-setup destroy\n command to destroy the instance.\n\n\nTo destroy a Layer0 instance:\n\n\n\n\n  \nAt the command prompt, type the following command, replacing _prefixName_ with the prefix you created when you created your Layer0 instance:\n    \n\n      \nl0-setup destroy\n \nprefixName\n\n    \n\n  \n\n\n\n\n\n\n  \nNote\n\n  \nThe \nl0-setup destroy\n operation is idempotent (that is, it has no additional effects if you execute it multiple times with the same parameters). Therefore, if the \ndestroy\n operation fails, you may be able to make it complete by running it again. If the \ndestroy\n operation continues to fail after running it again, please contact the Xfra team on our \nSlack channel\n.", 
            "title": "Destroy"
        }, 
        {
            "location": "/setup/destroy/#destroying-a-layer0-instance", 
            "text": "During testing or migration, you may find that you need to delete a non-functional or outdated instance of Layer0. This section provides procedures for destroying (deleting) a Layer0 instance.", 
            "title": "Destroying a Layer0 instance"
        }, 
        {
            "location": "/setup/destroy/#part-1-clean-up-your-layer0-environments", 
            "text": "In order to destroy a Layer0 instance, you must first delete all environments in the instance.  To delete Layer0 environments:  \n   At the command prompt, type the following command to see a list of environments in your Layer0 instance:\n     \n       l0 environment list \n      For each environment listed in the previous step, with the exception of the environments that begin with \"api\", issue the following command (where  environmentName  is the name of the environment you want to delete):\n     \n       l0 environment delete   environmentName   --wait \n     \n  Repeat this step until all of the environments, with the exception of the \"api\" environments, have been deleted.  At the command prompt, type the following command to list all of the certificates that exist in your Layer0 instance:\n     \n       l0 certificate list \n     \n   \n   For each certificate listed in the previous step, with the exception of the certificate named \"imshealthlabs\", type the following command to delete the certificate (replacing  certificateName  with the name of the certificate you want to delete):\n     \n       l0 certificate delete   certificateName   --wait \n     \n    Repeat this step until all of the certificates, with the exception of the \"imshealthlabs\" certificate, have been deleted. When you have finished deleting the environments and certificates in your Layer0 instance, proceed to Part 2.", 
            "title": "Part 1: Clean Up Your Layer0 Environments"
        }, 
        {
            "location": "/setup/destroy/#part-2-destroy-the-layer0-instance", 
            "text": "Once you have prepared your Layer0 instance for deletion, you can use the  l0-setup destroy  command to destroy the instance.  To destroy a Layer0 instance:  \n\n   At the command prompt, type the following command, replacing _prefixName_ with the prefix you created when you created your Layer0 instance:\n     \n       l0-setup destroy   prefixName \n     \n     \n   Note \n   The  l0-setup destroy  operation is idempotent (that is, it has no additional effects if you execute it multiple times with the same parameters). Therefore, if the  destroy  operation fails, you may be able to make it complete by running it again. If the  destroy  operation continues to fail after running it again, please contact the Xfra team on our  Slack channel .", 
            "title": "Part 2: Destroy the Layer0 instance"
        }, 
        {
            "location": "/guides/guestbook/", 
            "text": "Deployment guide: Guestbook sample application\n#\n\n\nIn this example, you will learn how different Layer0 commands work together to deploy web applications to the cloud. The sample application in this guide is a guestbook\na web application that acts as a simple message board.\n\n\nBefore you start\n#\n\n\nIn order to complete the procedures in this section, you must install and configure Layer0 v0.7.2 or later. If you have not already configured Layer0, see the \ninstallation guide\n. If you are running an older version of Layer0, see the \nupgrade instructions\n.\n\n\nOnce Layer0 is configured on your computer, download the \nGuestbook Task Definition\n; save the resulting file as \"Guestbook.Dockerrun.aws.json\". The instructions in this section assume that this file is located in the directory in which Layer0 is installed; if you place the file elsewhere, you will need to provide the complete path to the file when you reference it in \nPart 4\n.\n\n\n\n\nPart 1: Create the environment\n#\n\n\nThe first step in deploying an application in Layer0 is to create an environment.\nAn environment is a dedicated space in which one or more services can reside. In this example, you will create an environment named \"demo\".\n\n\nTo create the environment\n\n\n\n  \nAt the command prompt, run the following command to create a new environment named \ndemo\n:\n    \n\n      \nl0 environment create demo\n\n    \n\n  \nYou will see the following output:\n\nENVIRONMENT ID  ENVIRONMENT NAME  SERVICE ID\n1demo           demo\n\n  \n\n\n\n\nPart 2: Create the load balancer\n#\n\n\nIn order to expose a web application to the public internet, you must create a load balancer. A load balancer listens for web traffic at a specific address, and directs that traffic to a Layer0 service.\n\n\nBy default, Layer0 load balancers listen for web traffic on port 80 and forward it on port 80 using the TCP protocol. You can modify the way in which ports are forwarded, as well as the protocol used, using the \n--port\n option.\n\n\nIn this example, you will create a new load balancer called \"guestbooklb\" in the environment named \"demo\". The load balancer will listen on port 80, and forward traffic to port 80 in the Docker container using the HTTP protocol.\n\n\nTo create the load balancer:\n\n\n\n  \nAt the command prompt, run the following command to create a load balancer named \nguestbooklb\n that forwards traffic on port 80 using the HTTP protocol:\n    \n\n      \nl0 loadbalancer create --port 80:80/http demo guestbooklb\n\n    \n\n  \nYou will see the following output:\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS       PUBLIC  URL\n1guestbooklb     guestbooklb        demo                   80:80/http  true\n\n\n\n\n\n\nThe following is an summary of the arguments passed in the above command:\n\n\n\n\nloadbalancer create\n: creates a new load balancer\n\n\n--port 80:80/http\n: instructs the load balancer to forward requests from port 80 on the server to port 80 in the Docker container using the HTTP protocol\n\n\ndemo\n: the name of the environment in which you are creating the load balancer\n\n\nguestbooklb\n: a name for the load balancer itself\n\n\n\n\nPart 3: Deploy the Docker task definition\n#\n\n\nThe \ndeploy\n command is used to specify the Docker task definition that refers to a web application. In this section, you will create a new deploy called \"guestbook\" that refers to the Guestbook.Dockerrun.aws.json file you created earlier.\n\n\nTo deploy the Guestbook task definition:\n\n\n\n  \nAt the command prompt, run the following command:\n    \n\n      \nl0 deploy create Guestbook.Dockerrun.aws.json guestbook\n\n    \n\n  \nYou will see the following output:\n\nDEPLOY ID       DEPLOY NAME  VERSION\n1guestbook:1    guestbook    1\n\n  \n\n\n\n\nThe following is an summary of the arguments passed in the above command:\n\n\n\n\ndeploy create\n: creates a new deployment and allows you to specify a Docker task definition\n\n\nGuestbook.Dockerrun.aws.json\n: the file name of the Docker task definition (use the full path of the file if it is not in the same directory as l0-setup)\n\n\nguestbook\n: a name for the deploy, which you will use later when you create the service\n\n\n\n\nThe Deploy Name and Version are combined to create a unique identifier for a deploy.\nIf you create additional deploys named \"guestbook,\"  they will be assigned different version numbers.\n\n\nPart 4: Create the service\n#\n\n\nThe final part of the deployment process involves using the \nservice create\n command to create a new service and associate it with the environment, load balancer and deployment you created in the previous sections. The service will execute the containers described in the deployment. In this example, you will create a new service called \"guestbooksrv\".\n\n\nTo create the service:\n\n\n\n  \nAt the command prompt, run the following command:\n    \n\n      \nl0 service create --loadbalancer demo:guestbooklb demo guestbooksvc guestbook:latest\n\n    \n\n  \nYou will see the following output:\n\nSERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1guestbooksvc   guestbooksvc  demo         guestbooklb   guestbook:1  0/1\n\n  \n\n\n\n\nThe following is an summary of the arguments passed in the above command:\n\n\n\n\nservice create\n: creates a new service\n\n\n--loadbalancer demo:guestbooklb\n: the fully-qualified name of the load balancer; in this case, the load balancer named \"guestbooklb\" in the environment named \"demo.\" It is not strictly necessary to use the fully qualified name of the load balancer, unless another load balancer with exactly the same name exists in a different environment.\n\n\ndemo\n: the name of the environment you created in \nPart 1\n\n\nguestbooksvc\n: a name for the service you are creating\n\n\nguestbook\n: the name of the deploy that you created in \nPart 3\n\n\n\n\nPart 5: Test the application\n#\n\n\nCheck the status of the service\n#\n\n\nAfter you create a service, it may take several minutes for that service to completely finish deploying. You can check the status of a service using the \nservice get\n command.\n\n\nTo check the status:\n\n\n\n  \nAt the command prompt, type the following command to check the status of the \"guestbooksvc\" deploy:\n    \n\n      \nl0 service get demo:guestbooksvc\n\n    \n\n\n\n\nInitially, you will see an asterisk (*) next to the name of the \"guestbook:1\" deploy; this indicates that the service is in a transitional state. In this phase, if you execute the \nservice get\n command again, you will see the following output:\n\n\nSERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS       SCALE\n1guestbooksvc   guestbooksvc  demo         guestbooklb   guestbook:1*  0/1\n\n\n\n\nIn the next phase of the deployment, you will see \"(1)\" in the Scale column; this indicates that 1 copy of the service is transitioning to an active state. In this phase, if you execute the \nservice get\n command again, you will see the following output:\n\n\nSERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1guestbooksvc   guestbooksvc  demo         guestbooklb   guestbook:1  1/1\n\n\n\n\nGet the application URL\n#\n\n\nOnce the service has been completely deployed, you can obtain the URL for your application and launch it in a browser.\n\n\nTo test your web application:\n\n\n\n  \nAt the command prompt, type the following command:\n    \n\n      \nl0 loadbalancer get demo:guestbooklb\n\n    \n\n  You will see the following output:\n  \nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES            PORTS       PUBLIC  URL\n1guestbooklb     guestbooklb        demo         1guestbooksvc99706  80:80/HTTP  true    \nurl\n\n  \nCopy the value shown in the \nURL\n column and paste it into a web browser. The guestbook application will appear.", 
            "title": "Guestbook"
        }, 
        {
            "location": "/guides/guestbook/#deployment-guide-guestbook-sample-application", 
            "text": "In this example, you will learn how different Layer0 commands work together to deploy web applications to the cloud. The sample application in this guide is a guestbook a web application that acts as a simple message board.", 
            "title": "Deployment guide: Guestbook sample application"
        }, 
        {
            "location": "/guides/guestbook/#before-you-start", 
            "text": "In order to complete the procedures in this section, you must install and configure Layer0 v0.7.2 or later. If you have not already configured Layer0, see the  installation guide . If you are running an older version of Layer0, see the  upgrade instructions .  Once Layer0 is configured on your computer, download the  Guestbook Task Definition ; save the resulting file as \"Guestbook.Dockerrun.aws.json\". The instructions in this section assume that this file is located in the directory in which Layer0 is installed; if you place the file elsewhere, you will need to provide the complete path to the file when you reference it in  Part 4 .", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/guestbook/#part-1-create-the-environment", 
            "text": "The first step in deploying an application in Layer0 is to create an environment.\nAn environment is a dedicated space in which one or more services can reside. In this example, you will create an environment named \"demo\".  To create the environment  \n   At the command prompt, run the following command to create a new environment named  demo :\n     \n       l0 environment create demo \n     \n   You will see the following output: ENVIRONMENT ID  ENVIRONMENT NAME  SERVICE ID\n1demo           demo", 
            "title": "Part 1: Create the environment"
        }, 
        {
            "location": "/guides/guestbook/#part-2-create-the-load-balancer", 
            "text": "In order to expose a web application to the public internet, you must create a load balancer. A load balancer listens for web traffic at a specific address, and directs that traffic to a Layer0 service.  By default, Layer0 load balancers listen for web traffic on port 80 and forward it on port 80 using the TCP protocol. You can modify the way in which ports are forwarded, as well as the protocol used, using the  --port  option.  In this example, you will create a new load balancer called \"guestbooklb\" in the environment named \"demo\". The load balancer will listen on port 80, and forward traffic to port 80 in the Docker container using the HTTP protocol.  To create the load balancer:  \n   At the command prompt, run the following command to create a load balancer named  guestbooklb  that forwards traffic on port 80 using the HTTP protocol:\n     \n       l0 loadbalancer create --port 80:80/http demo guestbooklb \n     \n   You will see the following output: LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS       PUBLIC  URL\n1guestbooklb     guestbooklb        demo                   80:80/http  true    The following is an summary of the arguments passed in the above command:   loadbalancer create : creates a new load balancer  --port 80:80/http : instructs the load balancer to forward requests from port 80 on the server to port 80 in the Docker container using the HTTP protocol  demo : the name of the environment in which you are creating the load balancer  guestbooklb : a name for the load balancer itself", 
            "title": "Part 2: Create the load balancer"
        }, 
        {
            "location": "/guides/guestbook/#part-3-deploy-the-docker-task-definition", 
            "text": "The  deploy  command is used to specify the Docker task definition that refers to a web application. In this section, you will create a new deploy called \"guestbook\" that refers to the Guestbook.Dockerrun.aws.json file you created earlier.  To deploy the Guestbook task definition:  \n   At the command prompt, run the following command:\n     \n       l0 deploy create Guestbook.Dockerrun.aws.json guestbook \n     \n   You will see the following output: DEPLOY ID       DEPLOY NAME  VERSION\n1guestbook:1    guestbook    1 \n     The following is an summary of the arguments passed in the above command:   deploy create : creates a new deployment and allows you to specify a Docker task definition  Guestbook.Dockerrun.aws.json : the file name of the Docker task definition (use the full path of the file if it is not in the same directory as l0-setup)  guestbook : a name for the deploy, which you will use later when you create the service   The Deploy Name and Version are combined to create a unique identifier for a deploy.\nIf you create additional deploys named \"guestbook,\"  they will be assigned different version numbers.", 
            "title": "Part 3: Deploy the Docker task definition"
        }, 
        {
            "location": "/guides/guestbook/#part-4-create-the-service", 
            "text": "The final part of the deployment process involves using the  service create  command to create a new service and associate it with the environment, load balancer and deployment you created in the previous sections. The service will execute the containers described in the deployment. In this example, you will create a new service called \"guestbooksrv\".  To create the service:  \n   At the command prompt, run the following command:\n     \n       l0 service create --loadbalancer demo:guestbooklb demo guestbooksvc guestbook:latest \n     \n   You will see the following output: SERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1guestbooksvc   guestbooksvc  demo         guestbooklb   guestbook:1  0/1 \n     The following is an summary of the arguments passed in the above command:   service create : creates a new service  --loadbalancer demo:guestbooklb : the fully-qualified name of the load balancer; in this case, the load balancer named \"guestbooklb\" in the environment named \"demo.\" It is not strictly necessary to use the fully qualified name of the load balancer, unless another load balancer with exactly the same name exists in a different environment.  demo : the name of the environment you created in  Part 1  guestbooksvc : a name for the service you are creating  guestbook : the name of the deploy that you created in  Part 3", 
            "title": "Part 4: Create the service"
        }, 
        {
            "location": "/guides/guestbook/#part-5-test-the-application", 
            "text": "", 
            "title": "Part 5: Test the application"
        }, 
        {
            "location": "/guides/guestbook/#check-the-status-of-the-service", 
            "text": "After you create a service, it may take several minutes for that service to completely finish deploying. You can check the status of a service using the  service get  command.  To check the status:  \n   At the command prompt, type the following command to check the status of the \"guestbooksvc\" deploy:\n     \n       l0 service get demo:guestbooksvc \n       Initially, you will see an asterisk (*) next to the name of the \"guestbook:1\" deploy; this indicates that the service is in a transitional state. In this phase, if you execute the  service get  command again, you will see the following output:  SERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS       SCALE\n1guestbooksvc   guestbooksvc  demo         guestbooklb   guestbook:1*  0/1  In the next phase of the deployment, you will see \"(1)\" in the Scale column; this indicates that 1 copy of the service is transitioning to an active state. In this phase, if you execute the  service get  command again, you will see the following output:  SERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1guestbooksvc   guestbooksvc  demo         guestbooklb   guestbook:1  1/1", 
            "title": "Check the status of the service"
        }, 
        {
            "location": "/guides/guestbook/#get-the-application-url", 
            "text": "Once the service has been completely deployed, you can obtain the URL for your application and launch it in a browser.  To test your web application:  \n   At the command prompt, type the following command:\n     \n       l0 loadbalancer get demo:guestbooklb \n     \n  You will see the following output:\n   LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES            PORTS       PUBLIC  URL\n1guestbooklb     guestbooklb        demo         1guestbooksvc99706  80:80/HTTP  true     url \n   Copy the value shown in the  URL  column and paste it into a web browser. The guestbook application will appear.", 
            "title": "Get the application URL"
        }, 
        {
            "location": "/guides/guestbook_rds/", 
            "text": "Deployment guide: Guestbook with RDS Database\n#\n\n\nThe Guestbook application that you deployed in the \nGuestbook deployment guide\n was a very simple application that stored its data in memory (also known as a \"stateful\" application). If you were to re-deploy the Guestbook service, all of the data previously entered into the application would be lost permanently.\n\n\nA stateless application, on the other hand, does not record data generated in one session for use in subsequent sessions. In order to prevent data loss, web applications that you deploy using Layer0 should be stateless.\n\n\nThis guide will show you how to make a stateless Guestbook application that stores data in an Amazon Relational Database Service (RDS) database.\n\n\n\n\nBefore you start\n#\n\n\nIn order to complete the procedures in this section, you must install and configure Layer0 v0.7.0 or later. If you have not already configured Layer0, see the \ninstallation guide\n. If you are running an older version of Layer0, see the \nupgrade instructions\n.\n\n\nThis guide expands upon the \nGuestbook deployment guide\n deployment guide. You must complete the procedures in that guide before you can complete the procedures listed here. After completing the procedures in the Guestbook guide, your Layer0 should contain a service named \"guestbooksvc\", running a deploy named \"guestbook\", behind a load balancer named \"guestbooklb\", all within an environment named \"demo\".\n\n\nPart 1: Create an RDS instance\n#\n\n\nThe updated Guestbook application described in this guide stores its data in an Amazon Relational Database Service (RDS) database. These steps describe the process of creating a new RDS instance for this purpose.\n\n\nTo create a new RDS database:\n\n\n\n\nDownload the \nRDS Cloudformation script\n and save it to your computer.\n\n\nGo to \nthe AWS Console page for Layer0\n. Log in using your IMS Health domain credentials.\n\n\nUnder \nManagement Tools\n, click \nCloudFormation\n.\n\n\nClick \nCreate Stack\n.\n\n\nNext to \nChoose a Template\n, select \nUpload a template to Amazon S3\n. Click the \nBrowse\n button. On the file selection window, select the file that you downloaded in step 1 (\ncloudformation.json\n), and then click \nNext\n.\n\n\nUnder \nParameters\n, enter the following details:\n\n\nStack Name\n: \nLayer0Prefix\n-guestbook-rds\n\n\nDatabase\n: \nguestbook\n\n\nEnvironmentSG\n: Select the item that starts with \nLayer0Prefix\n-demo\n\n\nPassword\n: a password that you specify. The password must contain at least 8 characters. It cannot contain spaces or any of the following characters: /, @, \"\n\n\nPort\n: 3306\n\n\nUsername\n: a username that you specify.\n\n\nSubnets\n: Select the items that end with \n(layer0-\nLayer0Prefix\n-priA)\n and \n(layer0-\nLayer0Prefix\n-priB)\n\n\nVPC\n: Select the item that ends with \n(layer0-\nLayer0Prefix\n-vpc)\n\n\n\n\n\n\nClick \nNext\n until you arrive at the Review page. Click \nCreate\n. The process of creating the stack requires about 10 minutes to complete; wait until this process is complete before proceeding to the next section.\n\n\n\n\nPart 2: Create a deploy\n#\n\n\nAfter the RDS database is ready, you can create a new deploy for the service.\n\n\nTo create a new deploy:\n\n\n\n\nDownload the \nRDS Guestbook Task Definition\n and save it to your computer as \nGuestbookRDS.Dockerrun.aws.json\n.\n\n\nIn the AWS Console, under \nManagement Tools\n, click \nCloudFormation\n, and then click the name of the stack you created in the previous section. Click \nOutputs\n to see a list of the variables you specified in the previous section.\n\n\nIn a text editor, open the GuestbookRDS.Dockerrun.aws.json file. This file uses environment variables for configuration. Edit the file to use the appropriate values for your stack. When you are finished, the \nenvironment\n section should resemble the following example:\n\n\n...\n\"environment\": [\n    {\n        \"name\": \"GB_DB_NAME\",\n        \"value\": \"guestbook\"\n    },\n    {\n        \"name\": \"GB_DB_HOST\",\n        \"value\": \"guestbook.abc123.us-west-2.rds.amazonaws.com\"\n    },\n    {\n        \"name\": \"GB_DB_PORT\",\n        \"value\": \"3306\"\n    },\n    {\n        \"name\": \"GB_DB_USER\",\n        \"value\": \"admin\"\n    },\n    {\n        \"name\": \"GB_DB_PASSWORD\",\n        \"value\": \"password\"\n    }\n]\n...\n\n\n\nSave the changes you made to \nGuestbookRDS.Dockerrun.aws.json\n.\n\n\nAt the command prompt, run the following command:\n\n\n\n\nl0 deploy create GuestbookRDS.Dockerrun.aws.json guestbook\n\n\nYou will see the following output:\n\n\nDEPLOY ID       DEPLOY NAME  VERSION\n1guestbook:2    guestbook    2\n\n\n\n\nThe number 2 in the \nVersion\n column indicates that this is the second iteration of this deploy (the deploy from the previous guestbook example was the first iteration).\n\n\nPart 3: Apply the deploy\n#\n\n\nAt this point, you can use the \ndeploy\n command to deploy the latest version of the deploy named \"guestbook\" to our existing service (\"guestbooksvc\"). Using this command will instruct the service to run the new version of the Guestbook container, along with the environment variables you specified in the task definition in the previous section.\n\n\nTo apply the deploy, run the following command:\n\n\nl0 deploy apply guestbook:2 demo:guestbooksvc\n\n\nYou will see the following output:\n\n\nSuccessfully Applied Deploy\n\n\n\n\nPart 4: Test the application\n#\n\n\nCheck the status of the service\n#\n\n\nAfter you create a service, it may take several minutes for that service to completely finish deploying. You can check the status of a service using the \nservice get\n command.\n\n\nTo check the status of the guestbook service, run the following command:\n\n\n  \nl0 service get demo:guestbooksvc\n\n\n\nInitially, you will see an asterisk (*) next to the name of the \"guestbook:1\" deploy; this indicates that the service is in a transitional state. In this phase, if you execute the \nservice get\n command again, you will see the following output:\n\n\nSERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER     DEPLOYS       SCALE\n1guestbooksvc   guestbook     demo         guestbooklb      guestbook:2*  1/1\n                                                            guestbook:1\n\n\n\n\nIn the next phase of the deployment, you will see \"(1)\" in the Scale column; this indicates that 1 copy of the service is transitioning to an active state. In this phase, if you execute the \nservice get\n command again, you will see the following output:\n\n\nSERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER     DEPLOYS       SCALE\n1guestbooksvc   guestbook     demo         guestbooklb      guestbook:2*  1/1 (1)\n                                                            guestbook:1\n\n\n\n\nThe system will wait until the guestbook:2 deploy is running before it stops the currently-running guestbook:1 deploy. In this phase, if you execute the \nservice get\n command again, you will see the following output:\n\n\nSERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER     DEPLOYS       SCALE\n1guestbooksvc   guestbook     demo         guestbooklb      guestbook:2   2/1\n                                                            guestbook:1\n\n\n\n\nOnce the guestbook:2 deploy is running, the guestbook:1 deploy will be stopped.\nThe Deploys and Scale columns will indicate that only one deploy (the guestbook:2 deploy) is running. In this phase, if you execute the \nservice get\n command again, you will see the following output:\n\n\nSERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER     DEPLOYS      SCALE\n1guestbooksvc   guestbook     demo         guestbooklb      guestbook:2  1/1\n\n\n\n\nGet the application URL\n#\n\n\nOnce the service has been completely deployed, you can obtain the URL for your application and launch it in a browser.\n\n\nTo test the RDS guestbook application:\n\n\n\n  \nAt the command line, type the following command:\n    \n\n      \nl0 loadbalancer get demo:guestbooklb\n\n    \n\n  You will see the following output:\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES            PORTS       PUBLIC  URL\n1guestbooklb     guestbooklb        demo         1guestbooksvc99706  80:80/HTTP  true    \n(url)\n\n\n\n  \n\n  \nCopy the value shown in the \nURL\n column and paste it into a web browser. The RDS-backed guestbook application will appear.", 
            "title": "Guestbook with RDS"
        }, 
        {
            "location": "/guides/guestbook_rds/#deployment-guide-guestbook-with-rds-database", 
            "text": "The Guestbook application that you deployed in the  Guestbook deployment guide  was a very simple application that stored its data in memory (also known as a \"stateful\" application). If you were to re-deploy the Guestbook service, all of the data previously entered into the application would be lost permanently.  A stateless application, on the other hand, does not record data generated in one session for use in subsequent sessions. In order to prevent data loss, web applications that you deploy using Layer0 should be stateless.  This guide will show you how to make a stateless Guestbook application that stores data in an Amazon Relational Database Service (RDS) database.", 
            "title": "Deployment guide: Guestbook with RDS Database"
        }, 
        {
            "location": "/guides/guestbook_rds/#before-you-start", 
            "text": "In order to complete the procedures in this section, you must install and configure Layer0 v0.7.0 or later. If you have not already configured Layer0, see the  installation guide . If you are running an older version of Layer0, see the  upgrade instructions .  This guide expands upon the  Guestbook deployment guide  deployment guide. You must complete the procedures in that guide before you can complete the procedures listed here. After completing the procedures in the Guestbook guide, your Layer0 should contain a service named \"guestbooksvc\", running a deploy named \"guestbook\", behind a load balancer named \"guestbooklb\", all within an environment named \"demo\".", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/guestbook_rds/#part-1-create-an-rds-instance", 
            "text": "The updated Guestbook application described in this guide stores its data in an Amazon Relational Database Service (RDS) database. These steps describe the process of creating a new RDS instance for this purpose.  To create a new RDS database:   Download the  RDS Cloudformation script  and save it to your computer.  Go to  the AWS Console page for Layer0 . Log in using your IMS Health domain credentials.  Under  Management Tools , click  CloudFormation .  Click  Create Stack .  Next to  Choose a Template , select  Upload a template to Amazon S3 . Click the  Browse  button. On the file selection window, select the file that you downloaded in step 1 ( cloudformation.json ), and then click  Next .  Under  Parameters , enter the following details:  Stack Name :  Layer0Prefix -guestbook-rds  Database :  guestbook  EnvironmentSG : Select the item that starts with  Layer0Prefix -demo  Password : a password that you specify. The password must contain at least 8 characters. It cannot contain spaces or any of the following characters: /, @, \"  Port : 3306  Username : a username that you specify.  Subnets : Select the items that end with  (layer0- Layer0Prefix -priA)  and  (layer0- Layer0Prefix -priB)  VPC : Select the item that ends with  (layer0- Layer0Prefix -vpc)    Click  Next  until you arrive at the Review page. Click  Create . The process of creating the stack requires about 10 minutes to complete; wait until this process is complete before proceeding to the next section.", 
            "title": "Part 1: Create an RDS instance"
        }, 
        {
            "location": "/guides/guestbook_rds/#part-2-create-a-deploy", 
            "text": "After the RDS database is ready, you can create a new deploy for the service.  To create a new deploy:   Download the  RDS Guestbook Task Definition  and save it to your computer as  GuestbookRDS.Dockerrun.aws.json .  In the AWS Console, under  Management Tools , click  CloudFormation , and then click the name of the stack you created in the previous section. Click  Outputs  to see a list of the variables you specified in the previous section.  In a text editor, open the GuestbookRDS.Dockerrun.aws.json file. This file uses environment variables for configuration. Edit the file to use the appropriate values for your stack. When you are finished, the  environment  section should resemble the following example: \n...\n\"environment\": [\n    {\n        \"name\": \"GB_DB_NAME\",\n        \"value\": \"guestbook\"\n    },\n    {\n        \"name\": \"GB_DB_HOST\",\n        \"value\": \"guestbook.abc123.us-west-2.rds.amazonaws.com\"\n    },\n    {\n        \"name\": \"GB_DB_PORT\",\n        \"value\": \"3306\"\n    },\n    {\n        \"name\": \"GB_DB_USER\",\n        \"value\": \"admin\"\n    },\n    {\n        \"name\": \"GB_DB_PASSWORD\",\n        \"value\": \"password\"\n    }\n]\n...  Save the changes you made to  GuestbookRDS.Dockerrun.aws.json .  At the command prompt, run the following command:   l0 deploy create GuestbookRDS.Dockerrun.aws.json guestbook  You will see the following output:  DEPLOY ID       DEPLOY NAME  VERSION\n1guestbook:2    guestbook    2  The number 2 in the  Version  column indicates that this is the second iteration of this deploy (the deploy from the previous guestbook example was the first iteration).", 
            "title": "Part 2: Create a deploy"
        }, 
        {
            "location": "/guides/guestbook_rds/#part-3-apply-the-deploy", 
            "text": "At this point, you can use the  deploy  command to deploy the latest version of the deploy named \"guestbook\" to our existing service (\"guestbooksvc\"). Using this command will instruct the service to run the new version of the Guestbook container, along with the environment variables you specified in the task definition in the previous section.  To apply the deploy, run the following command:  l0 deploy apply guestbook:2 demo:guestbooksvc  You will see the following output:  Successfully Applied Deploy", 
            "title": "Part 3: Apply the deploy"
        }, 
        {
            "location": "/guides/guestbook_rds/#part-4-test-the-application", 
            "text": "", 
            "title": "Part 4: Test the application"
        }, 
        {
            "location": "/guides/guestbook_rds/#check-the-status-of-the-service", 
            "text": "After you create a service, it may take several minutes for that service to completely finish deploying. You can check the status of a service using the  service get  command.  To check the status of the guestbook service, run the following command: \n   l0 service get demo:guestbooksvc  \nInitially, you will see an asterisk (*) next to the name of the \"guestbook:1\" deploy; this indicates that the service is in a transitional state. In this phase, if you execute the  service get  command again, you will see the following output:  SERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER     DEPLOYS       SCALE\n1guestbooksvc   guestbook     demo         guestbooklb      guestbook:2*  1/1\n                                                            guestbook:1  In the next phase of the deployment, you will see \"(1)\" in the Scale column; this indicates that 1 copy of the service is transitioning to an active state. In this phase, if you execute the  service get  command again, you will see the following output:  SERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER     DEPLOYS       SCALE\n1guestbooksvc   guestbook     demo         guestbooklb      guestbook:2*  1/1 (1)\n                                                            guestbook:1  The system will wait until the guestbook:2 deploy is running before it stops the currently-running guestbook:1 deploy. In this phase, if you execute the  service get  command again, you will see the following output:  SERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER     DEPLOYS       SCALE\n1guestbooksvc   guestbook     demo         guestbooklb      guestbook:2   2/1\n                                                            guestbook:1  Once the guestbook:2 deploy is running, the guestbook:1 deploy will be stopped.\nThe Deploys and Scale columns will indicate that only one deploy (the guestbook:2 deploy) is running. In this phase, if you execute the  service get  command again, you will see the following output:  SERVICE ID      SERVICE NAME  ENVIRONMENT  LOADBALANCER     DEPLOYS      SCALE\n1guestbooksvc   guestbook     demo         guestbooklb      guestbook:2  1/1", 
            "title": "Check the status of the service"
        }, 
        {
            "location": "/guides/guestbook_rds/#get-the-application-url", 
            "text": "Once the service has been completely deployed, you can obtain the URL for your application and launch it in a browser.  To test the RDS guestbook application:  \n   At the command line, type the following command:\n     \n       l0 loadbalancer get demo:guestbooklb \n     \n  You will see the following output: LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES            PORTS       PUBLIC  URL\n1guestbooklb     guestbooklb        demo         1guestbooksvc99706  80:80/HTTP  true     (url)  \n   \n   Copy the value shown in the  URL  column and paste it into a web browser. The RDS-backed guestbook application will appear.", 
            "title": "Get the application URL"
        }, 
        {
            "location": "/guides/consul/", 
            "text": "Deployment guide: Consul service\n#\n\n\nConsul is a tool for configuring services in your infrastructure. It includes several features, including health monitoring, service discovery and key/value storage.\n\n\nThis guide provides step-by-step instructions for deploying Consul in a Layer0 instance. These procedures build upon the \nGuestbook\n and \nGuestbook with RDS\n deployment guides; you must complete the procedures in those guides before you can complete the procedures in this guide.\n\n\nBefore you start\n#\n\n\nIn order to complete the procedures in this section, you must install and configure Layer0 v0.7.0 or later. If you have not already configured Layer0, see the \ninstallation guide\n. If you are running an older version of Layer0, see the \nupgrade instructions\n.\n\n\nThis guide expands upon the \nGuestbook with RDS Database deployment guide\n. The procedures in this guide assume that you completed the Guestbook with RDS deployment guide and all of its prerequisites.\n\n\nPart 1: Create a load balancer\n#\n\n\nConsul should run behind a private load balancer in the \ndemo\n environment with ports 8500 and 8301 exposed.\n\n\nTo create the load balancer:\n\n\n\n  \nAt the command line, type the following command to create a load balancer named \nconsullb\n in the \ndemo\n environment with port 8500 exposed:\n    \n\n      \nl0 loadbalancer create --private --port 8500:8500/tcp demo consullb\n\n    \nYou will see the following output:\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS          PUBLIC  URL\n1consullb        consullb           demo                   8500:8500/tcp  false \n\n\n\n  \n\n  \nAt the command line, type the following command to add port 8301 to the \nconsullb\n load balancer:\n    \n\n      \nl0 loadbalancer addport demo:consullb 8301:8301/tcp\n\n    \n\n  You will see the following output:\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS          PUBLIC  URL\n1consullb        consullb           demo                   8500:8500/tcp  false   \n(url)\n\n                                                           8301:8301/tcp\n\n\n\n\nCopy the URL listed in the \nURL\n column; you will need this URL in the next section.\n\n\n\n\nPart 2: Configure the deploy\n#\n\n\n\n  \nDownload the \nConsul Task Definition\n and save it to your computer as Consul.Dockerrun.aws.json.\n\n  \nOpen Consul.Dockerrun.aws.json in a text editor. Toward the bottom of the file, you will see the following:\n\n\"environment\": [\n    {\n        \"name\": \"EXTERNAL_URL\",\n        \"value\": \"\nurl\n\"\n     }\n]\n\n\n\n\nReplace \nurl\n with the URL you copied in step 2 of the previous section, and then save the file.\n\n  \nAt the command line, type the following command to create a new deploy called \nconsul\n:\n    \n\n      \nl0 deploy create Consul.Dockerrun.aws.json consul\n\n    \n\n  You will see the following output:\n\nDEPLOY ID  DEPLOY NAME  VERSION\n\n1consul:1  consul       1      \n\n\n\n  \n\n\n\n\nPart 3: Create the service\n#\n\n\nNow that you've created an environment, load balancer, and deploy, you can create a service to bring these elements together.\n\n\nTo create the service:\n\n\n\n  \nAt the command line, type the following command to create a new service called \nconsul\n:\n    \n\n      \nl0 service create --loadbalancer demo:consullb demo consulsvc consul:latest\n\n    \n\n    You will see the following output:\n\nSERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     0/1\n\n  \n\n  \nWait several minutes for the service to be provisioned. You can check the status of the service creation by running the following command:\n  \n\n    \nl0 service get consulsvc\n\n  \n\n  When the service has finished provisioning, you will see the following output:\n\n\nSERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     1/1\n\n\n\n\n\n\n\n\nPart 4: Scale the Consul service\n#\n\n\nConsul is a scalable application. For added reliability, we recommend that you scale the Consul service to size 3.\n\n\nTo scale the service:\n\n\n\n  \nAt the command line, type the following to scale the consul service to size 3:\n    \n\n      \nl0 service scale demo:consulsvc 3\n\n    \n\n  \n\n\n  \nWait several minutes for the service to scale. You can check the status of the service by running the following command:\n  \n\n    \nl0 service get consul\n\n  \n\n  When the Service has finished scaling, you will see the following output:\n\nSERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     3/3\n\n\n  \n\n\n\n\n\nAdditional steps: Configure Layer0 services\n#\n\n\nIn order to for Layer0 services to use consul, the task definitions for those services must be configured. The next deployment guide in this series (\nGuestbook with Consul\n) contains an example of these configurations.", 
            "title": "Consul"
        }, 
        {
            "location": "/guides/consul/#deployment-guide-consul-service", 
            "text": "Consul is a tool for configuring services in your infrastructure. It includes several features, including health monitoring, service discovery and key/value storage.  This guide provides step-by-step instructions for deploying Consul in a Layer0 instance. These procedures build upon the  Guestbook  and  Guestbook with RDS  deployment guides; you must complete the procedures in those guides before you can complete the procedures in this guide.", 
            "title": "Deployment guide: Consul service"
        }, 
        {
            "location": "/guides/consul/#before-you-start", 
            "text": "In order to complete the procedures in this section, you must install and configure Layer0 v0.7.0 or later. If you have not already configured Layer0, see the  installation guide . If you are running an older version of Layer0, see the  upgrade instructions .  This guide expands upon the  Guestbook with RDS Database deployment guide . The procedures in this guide assume that you completed the Guestbook with RDS deployment guide and all of its prerequisites.", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/consul/#part-1-create-a-load-balancer", 
            "text": "Consul should run behind a private load balancer in the  demo  environment with ports 8500 and 8301 exposed.  To create the load balancer:  \n   At the command line, type the following command to create a load balancer named  consullb  in the  demo  environment with port 8500 exposed:\n     \n       l0 loadbalancer create --private --port 8500:8500/tcp demo consullb \n     You will see the following output: LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS          PUBLIC  URL\n1consullb        consullb           demo                   8500:8500/tcp  false   \n   \n   At the command line, type the following command to add port 8301 to the  consullb  load balancer:\n     \n       l0 loadbalancer addport demo:consullb 8301:8301/tcp \n     \n  You will see the following output: LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS          PUBLIC  URL\n1consullb        consullb           demo                   8500:8500/tcp  false    (url) \n                                                           8301:8301/tcp  \nCopy the URL listed in the  URL  column; you will need this URL in the next section.", 
            "title": "Part 1: Create a load balancer"
        }, 
        {
            "location": "/guides/consul/#part-2-configure-the-deploy", 
            "text": "Download the  Consul Task Definition  and save it to your computer as Consul.Dockerrun.aws.json. \n   Open Consul.Dockerrun.aws.json in a text editor. Toward the bottom of the file, you will see the following: \"environment\": [\n    {\n        \"name\": \"EXTERNAL_URL\",\n        \"value\": \" url \"\n     }\n]  \nReplace  url  with the URL you copied in step 2 of the previous section, and then save the file. \n   At the command line, type the following command to create a new deploy called  consul :\n     \n       l0 deploy create Consul.Dockerrun.aws.json consul \n     \n  You will see the following output: DEPLOY ID  DEPLOY NAME  VERSION \n1consul:1  consul       1", 
            "title": "Part 2: Configure the deploy"
        }, 
        {
            "location": "/guides/consul/#part-3-create-the-service", 
            "text": "Now that you've created an environment, load balancer, and deploy, you can create a service to bring these elements together.  To create the service:  \n   At the command line, type the following command to create a new service called  consul :\n     \n       l0 service create --loadbalancer demo:consullb demo consulsvc consul:latest \n     \n    You will see the following output: SERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     0/1 \n   \n   Wait several minutes for the service to be provisioned. You can check the status of the service creation by running the following command:\n   \n     l0 service get consulsvc \n   \n  When the service has finished provisioning, you will see the following output:  SERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     1/1", 
            "title": "Part 3: Create the service"
        }, 
        {
            "location": "/guides/consul/#part-4-scale-the-consul-service", 
            "text": "Consul is a scalable application. For added reliability, we recommend that you scale the Consul service to size 3.  To scale the service:  \n   At the command line, type the following to scale the consul service to size 3:\n     \n       l0 service scale demo:consulsvc 3 \n     \n   \n\n   Wait several minutes for the service to scale. You can check the status of the service by running the following command:\n   \n     l0 service get consul \n   \n  When the Service has finished scaling, you will see the following output: SERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1consul     consulsvc     demo         consullb      consul:1     3/3", 
            "title": "Part 4: Scale the Consul service"
        }, 
        {
            "location": "/guides/consul/#additional-steps-configure-layer0-services", 
            "text": "In order to for Layer0 services to use consul, the task definitions for those services must be configured. The next deployment guide in this series ( Guestbook with Consul ) contains an example of these configurations.", 
            "title": "Additional steps: Configure Layer0 services"
        }, 
        {
            "location": "/guides/guestbook_consul/", 
            "text": "Deployment guide: Guestbook with Consul\n#\n\n\nThis guide provides step-by-step instructions for deploying a Guestbook application that stores data in a\n\nRedis\n database. The Guestbook application uses \nConsul\n to dynamically discover the Redis service.\n\n\nBefore you start\n#\n\n\nThis guide assumes that you are running Layer0 version 0.7.2 or later, and that you have completed the\n\nGuestbook\n and \nConsul\n deployment guides.\n\n\nPart 1: Configure and deploy the Redis task definition\n#\n\n\nThe updated Guestbook application in this guide stores its data in a Redis database. Before you can deploy the updated Guestbook application, you must first configure and deploy the Redis task definition.\n\n\nTo configure and deploy the task definition:\n\n\n\n  \nDownload the \nRedis task\ndefinition\n and save it to your computer as Redis.dockerrun.aws.json.\n\n  \nAt the command prompt, type the following command:\n    \n\n      \nl0 loadbalancer get consul\n\n    \n\n    Copy the value in the \nURL\n column; you will need it in the next step.\n  \n\n  \nOpen Redis.dockerrun.aws.json in a text editor. Toward the end of the file, you will see the following:\n\n\"environment\": [\n    {\n        \"name\": \"EXTERNAL_URL\",\n        \"value\": \"\nurl\n\"\n    }\n]\n\n    In this section, replace \nurl\n with the URL you copied in the previous step.\n  \n\n  \nAt the command prompt, type the following command to create a new deploy named \nredis\n that uses the Redis.dockerrun.aws task definition:\n    \n\n      \nl0 deploy create Redis.Dockerrun.aws.json redis\n\n    \n\n  You will see the following output:\n\nDEPLOY ID  DEPLOY NAME  VERSION\n\n1redis:1   redis        1\n\n  \n\n\n\n\nPart 2: Create the redis service\n#\n\n\nNow that you have created the \nredis\n deploy, you can create a service to place it in.\n\n\nTo create a new service:\n\n\n\n  \nAt the command line, type the following command to create a service named \nredis\n running the deploy you created in the previous section:\n    \n\n      \n \nl0 service create demo redis redis:latest\n\n    \n\n    When you execute this command, the \nRegistrator\n service will read the environment variables for the l0-demo-redis container. These variables include the following:\n\n\nenvironment\n: [\n    {\n        \nname\n: \nSERVICE_NAME\n,\n        \nvalue\n: \ndb\n\n    },\n    {\n        \nname\n: \nSERVICE_TAGS\n,\n        \nvalue\n: \nguestbook\n\n    },\n    ...\n\n\n\n\nAfter reading these variables, Registrator will register a service named \ndb\n into your environment's Consul service.\nMembers of the Consul Cluster will be able to discover this service via DNS queries to guestbook.db.service.consul.\n  \n\n\n\n\nPart 3: Update the Guestbook service\n#\n\n\nTo configure the Guestbook application to use Consul for service discovery, you must first update the \nguestbook\n deploy.\n\n\nTo update the Guestbook service:\n\n\n\n  \nDownload the \nGuestbook with Consul Task definition\n and save it to your computer as GuestbookConsul.Dockerrun.aws.json.\n\n  \nAt the command line, type the following command: \nl0 loadbalancer get consullb\n. Copy the value in the \nURL\n column.\n\n  \nOpen GuestbookConsul.Dockerrun.aws.json in a text editor. Toward the bottom of the file, in the \nenvironment\n section, replace \nurl\n with the URL\nthat you copied in the previous step. Save the file.\n\n  \nAt the command line, type the following command to create a new version of the guestbook deploy using the updated Guestbook task definition:\n    \n\n      \nl0 deploy create GuestbookConsul.Dockerrun.aws.json guestbook\n\n    \n\n  You will see the following output:\n\n\nDEPLOY ID     DEPLOY NAME  VERSION\n1guestbook:3  guestbook    3\n\n\n  \n\n  \nAt the command line, type the following command to apply the updated \nguestbook\n deploy on the \nguestbook\n service:\n    \n\n      \nl0 deploy apply guestbook:latest demo:guestbook\n\n    \n\n  When you execute this command, the Guestbook application will automatically discover the Redis service by making a DNS query to guestbook.db.service.consul.\n\n  \nAt the command prompt, type the following command to find the URL of the \nguestbooklb\n load balancer:\n    \n\n      \nl0 loadbalancer get guestbooklb\n\n    \n\n  If the service has not finished deploying, you may see the following message:\n\"Could not connect to redis server, try refreshing.\"\n\nIf you see this message, wait a few minutes, and then refresh the page. You can also try using your web browser's Incognito or Private Browsing mode to ensure that the page is not cached.", 
            "title": "Guestbook with Consul"
        }, 
        {
            "location": "/guides/guestbook_consul/#deployment-guide-guestbook-with-consul", 
            "text": "This guide provides step-by-step instructions for deploying a Guestbook application that stores data in a Redis  database. The Guestbook application uses  Consul  to dynamically discover the Redis service.", 
            "title": "Deployment guide: Guestbook with Consul"
        }, 
        {
            "location": "/guides/guestbook_consul/#before-you-start", 
            "text": "This guide assumes that you are running Layer0 version 0.7.2 or later, and that you have completed the Guestbook  and  Consul  deployment guides.", 
            "title": "Before you start"
        }, 
        {
            "location": "/guides/guestbook_consul/#part-1-configure-and-deploy-the-redis-task-definition", 
            "text": "The updated Guestbook application in this guide stores its data in a Redis database. Before you can deploy the updated Guestbook application, you must first configure and deploy the Redis task definition.  To configure and deploy the task definition:  \n   Download the  Redis task\ndefinition  and save it to your computer as Redis.dockerrun.aws.json. \n   At the command prompt, type the following command:\n     \n       l0 loadbalancer get consul \n     \n    Copy the value in the  URL  column; you will need it in the next step.\n   \n   Open Redis.dockerrun.aws.json in a text editor. Toward the end of the file, you will see the following: \"environment\": [\n    {\n        \"name\": \"EXTERNAL_URL\",\n        \"value\": \" url \"\n    }\n] \n    In this section, replace  url  with the URL you copied in the previous step.\n   \n   At the command prompt, type the following command to create a new deploy named  redis  that uses the Redis.dockerrun.aws task definition:\n     \n       l0 deploy create Redis.Dockerrun.aws.json redis \n     \n  You will see the following output: DEPLOY ID  DEPLOY NAME  VERSION \n1redis:1   redis        1", 
            "title": "Part 1: Configure and deploy the Redis task definition"
        }, 
        {
            "location": "/guides/guestbook_consul/#part-2-create-the-redis-service", 
            "text": "Now that you have created the  redis  deploy, you can create a service to place it in.  To create a new service:  \n   At the command line, type the following command to create a service named  redis  running the deploy you created in the previous section:\n     \n         l0 service create demo redis redis:latest \n     \n    When you execute this command, the  Registrator  service will read the environment variables for the l0-demo-redis container. These variables include the following:  environment : [\n    {\n         name :  SERVICE_NAME ,\n         value :  db \n    },\n    {\n         name :  SERVICE_TAGS ,\n         value :  guestbook \n    },\n    ...  After reading these variables, Registrator will register a service named  db  into your environment's Consul service.\nMembers of the Consul Cluster will be able to discover this service via DNS queries to guestbook.db.service.consul.", 
            "title": "Part 2: Create the redis service"
        }, 
        {
            "location": "/guides/guestbook_consul/#part-3-update-the-guestbook-service", 
            "text": "To configure the Guestbook application to use Consul for service discovery, you must first update the  guestbook  deploy.  To update the Guestbook service:  \n   Download the  Guestbook with Consul Task definition  and save it to your computer as GuestbookConsul.Dockerrun.aws.json. \n   At the command line, type the following command:  l0 loadbalancer get consullb . Copy the value in the  URL  column. \n   Open GuestbookConsul.Dockerrun.aws.json in a text editor. Toward the bottom of the file, in the  environment  section, replace  url  with the URL\nthat you copied in the previous step. Save the file. \n   At the command line, type the following command to create a new version of the guestbook deploy using the updated Guestbook task definition:\n     \n       l0 deploy create GuestbookConsul.Dockerrun.aws.json guestbook \n     \n  You will see the following output: DEPLOY ID     DEPLOY NAME  VERSION\n1guestbook:3  guestbook    3 \n   \n   At the command line, type the following command to apply the updated  guestbook  deploy on the  guestbook  service:\n     \n       l0 deploy apply guestbook:latest demo:guestbook \n     \n  When you execute this command, the Guestbook application will automatically discover the Redis service by making a DNS query to guestbook.db.service.consul. \n   At the command prompt, type the following command to find the URL of the  guestbooklb  load balancer:\n     \n       l0 loadbalancer get guestbooklb \n     \n  If the service has not finished deploying, you may see the following message:\n\"Could not connect to redis server, try refreshing.\" \nIf you see this message, wait a few minutes, and then refresh the page. You can also try using your web browser's Incognito or Private Browsing mode to ensure that the page is not cached.", 
            "title": "Part 3: Update the Guestbook service"
        }, 
        {
            "location": "/guides/shinken/", 
            "text": "Deployment guide: Shinken\n#\n\n\nShinken is an open-source application monitoring framework that provides several benefits, including:\n\n\n\n\nExternally-accessible status URLs\n\n\nHealth checks for applications that use Consul\n\n\nSlack channel integration\n\n\n\n\nThis guide provides instructions for deploying a Shinken service using Layer0.\n\n\nBefore You Start\n#\n\n\nThis guide assumes you have an instance of Layer0 version v0.7.0 or higher; if not, complete the \nInstallation Instructions\n.\n\n\nOnce Layer0 is configured on your computer, download the \nShinken Task Definition\n; name the resulting file \nShinken.Dockerrun.aws.json\n.\n\n\nThis guide expands upon the \nGuestbook with Consul\n deployment guide. You must complete the procedures in that guide before you can complete the procedures listed here.\n\n\nPart 1: Create the load balancer\n#\n\n\nWhen run as a Layer0 service, Shinken should be placed behind a public load balancer. The load balancer should transfer traffic via port 80 over the HTTP protocol.\n\n\nTo create the load balancer:\n\n\n\n  \nAt the command prompt, type the following command to create a new load balancer named **shinkenlb** in the **demo** environment that forwards traffic through port 80 via the HTTP protocol:\n    \n\n      \nl0 loadbalancer create --port 80:80/http demo shinkenlb\n\n    \n\nYou will see the following output:\n\nLOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS       PUBLIC  URL\n\n(id)\n             shinkenlb          demo                   80:80/http  false   \n(url)\n\n  \n\n\n\n\n\nPart 2: Configure the deploy\n#\n\n\nBefore you can deploy Shinken, you must modify the Shinken task definition.\n\n\nTo configure the Shinken task definition:\n\n\n\n  \nIn a text editor, open Shinken.Dockerrun.aws.json.\n\n  \nToward the bottom of the file, in the \nenvironment\n section, you will see the following:\n  \n\"environment\": [\n    {\n        \"name\": \"SLACK_WEBHOOK_URL\",\n        \"value\": \"\"\n    },\n    {\n        \"name\": \"EXTERNAL_URL\",\n        \"value\": \"\"\n    },\n    {\n        \"name\": \"ADMIN_PASSWORD_HASH\",\n        \"value\": \"\"\n    }\n]\n\n  Modify the file to include the following values:\n  \n\n    \nSLACK_WEBHOOK_URL\n: The incoming webhook URL for your team's Slack channel.\n      \n\n        \nTo find the webhook URL for your channel, visit the \nIncoming WebHooks\n page, and then click the name of the channel. Copy the value shown in the \nWebhook URL\n field.\n\n        \nAlternatively, if an incoming webhook does not already exist for your channel, click the \nAdd Configuration\n button to create a new incoming webhook integration.\n\n        \nThe Xfra team has created a #test channel in Slack for testing webhooks. The webhook URL for the #test channel is https://hooks.slack.com/services/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\n\n      \n\n    \n\n    \nEXTERNAL_URL\n: The URL for the \nshinkenlb\n load balancer. To find the URL, type the following command, and then copy the value in the \nURL\n column:\n      \n\n        \nl0 loadbalancer get shinkenlb\n\n      \n\n    \n\n    \nADMIN_PASSWORD_HASH\n: A hashed password that you will use to access the Shinken user interface, in the format \nadmin:\nhash\n\n      \n\n        \nLinux and Mac users can type the following command in a terminal window to obtain a password hash:\n        \n\n          \nhtpasswd -nb admin\n \nyourPassword\n\n        \n\n        \nWindows users should use \nan online htpasswd generator\n to create the password hash.\n\n      \n\n    \n\n    When you have finished making changes, save the file.\n  \n\n  \nType the following command to create a deploy named \nshinken\n using the task definition you just modified:\n    \n\n      \nl0 deploy create Shinken.Dockerrun.aws.json shinken\n\n    \n\n    You will see the following output:\n\nDEPLOY ID   DEPLOY NAME  VERSION\n1shinken:1  shinken      1\n\n  \n\n\n\n\nPart 3: Create the service\n#\n\n\nNow that you have created the \nshinken\n deploy, you can use the \nservice\n command to run it.\n\n\nTo create the service:\n\n\n\n  \nAt the command prompt, type the following command to create a service named \nshinkensvc\n that uses the \nshinkenlb\n load balancer:\n    \n\n      \nl0 service create --loadbalancer demo:shinkenlb demo shinkensvc shinken:latest\n\n    \n\n    You will see the following output:\n\nSERVICE ID   SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1shinken     shinkensvc    demo         shinkenlb     shinken:1    0/1\n\n\n  \n\n  \nWait several minutes for the service to provision completely. Use the following command to check the status of the service provisioning:\n    \n\n      \nl0 service get demo:shinkensvc\n\n    \n\n    When the service is ready, you will see the following output:\n\nSERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n\n(id)\n        shinkensvc    demo         shinkenlb     shinken:1    1/1\n\n  \n\n\n\n\nAdditional steps: Configuring other services\n#\n\n\nIn order to use Shinken health checks, you will need to modify the task definitions for your Layer0 applications. For more information, see the \nShinken Reference page\n.", 
            "title": "Shinken"
        }, 
        {
            "location": "/guides/shinken/#deployment-guide-shinken", 
            "text": "Shinken is an open-source application monitoring framework that provides several benefits, including:   Externally-accessible status URLs  Health checks for applications that use Consul  Slack channel integration   This guide provides instructions for deploying a Shinken service using Layer0.", 
            "title": "Deployment guide: Shinken"
        }, 
        {
            "location": "/guides/shinken/#before-you-start", 
            "text": "This guide assumes you have an instance of Layer0 version v0.7.0 or higher; if not, complete the  Installation Instructions .  Once Layer0 is configured on your computer, download the  Shinken Task Definition ; name the resulting file  Shinken.Dockerrun.aws.json .  This guide expands upon the  Guestbook with Consul  deployment guide. You must complete the procedures in that guide before you can complete the procedures listed here.", 
            "title": "Before You Start"
        }, 
        {
            "location": "/guides/shinken/#part-1-create-the-load-balancer", 
            "text": "When run as a Layer0 service, Shinken should be placed behind a public load balancer. The load balancer should transfer traffic via port 80 over the HTTP protocol.  To create the load balancer:  \n   At the command prompt, type the following command to create a new load balancer named **shinkenlb** in the **demo** environment that forwards traffic through port 80 via the HTTP protocol:\n     \n       l0 loadbalancer create --port 80:80/http demo shinkenlb \n     \nYou will see the following output: LOADBALANCER ID  LOADBALANCER NAME  ENVIRONMENT  SERVICES  PORTS       PUBLIC  URL (id)              shinkenlb          demo                   80:80/http  false    (url)", 
            "title": "Part 1: Create the load balancer"
        }, 
        {
            "location": "/guides/shinken/#part-2-configure-the-deploy", 
            "text": "Before you can deploy Shinken, you must modify the Shinken task definition.  To configure the Shinken task definition:  \n   In a text editor, open Shinken.Dockerrun.aws.json. \n   Toward the bottom of the file, in the  environment  section, you will see the following:\n   \"environment\": [\n    {\n        \"name\": \"SLACK_WEBHOOK_URL\",\n        \"value\": \"\"\n    },\n    {\n        \"name\": \"EXTERNAL_URL\",\n        \"value\": \"\"\n    },\n    {\n        \"name\": \"ADMIN_PASSWORD_HASH\",\n        \"value\": \"\"\n    }\n] \n  Modify the file to include the following values:\n   \n     SLACK_WEBHOOK_URL : The incoming webhook URL for your team's Slack channel.\n       \n         To find the webhook URL for your channel, visit the  Incoming WebHooks  page, and then click the name of the channel. Copy the value shown in the  Webhook URL  field. \n         Alternatively, if an incoming webhook does not already exist for your channel, click the  Add Configuration  button to create a new incoming webhook integration. \n         The Xfra team has created a #test channel in Slack for testing webhooks. The webhook URL for the #test channel is https://hooks.slack.com/services/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 \n       \n     \n     EXTERNAL_URL : The URL for the  shinkenlb  load balancer. To find the URL, type the following command, and then copy the value in the  URL  column:\n       \n         l0 loadbalancer get shinkenlb \n       \n     \n     ADMIN_PASSWORD_HASH : A hashed password that you will use to access the Shinken user interface, in the format  admin: hash \n       \n         Linux and Mac users can type the following command in a terminal window to obtain a password hash:\n         \n           htpasswd -nb admin   yourPassword \n         \n         Windows users should use  an online htpasswd generator  to create the password hash. \n       \n     \n    When you have finished making changes, save the file.\n   \n   Type the following command to create a deploy named  shinken  using the task definition you just modified:\n     \n       l0 deploy create Shinken.Dockerrun.aws.json shinken \n     \n    You will see the following output: DEPLOY ID   DEPLOY NAME  VERSION\n1shinken:1  shinken      1", 
            "title": "Part 2: Configure the deploy"
        }, 
        {
            "location": "/guides/shinken/#part-3-create-the-service", 
            "text": "Now that you have created the  shinken  deploy, you can use the  service  command to run it.  To create the service:  \n   At the command prompt, type the following command to create a service named  shinkensvc  that uses the  shinkenlb  load balancer:\n     \n       l0 service create --loadbalancer demo:shinkenlb demo shinkensvc shinken:latest \n     \n    You will see the following output: SERVICE ID   SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE\n1shinken     shinkensvc    demo         shinkenlb     shinken:1    0/1 \n   \n   Wait several minutes for the service to provision completely. Use the following command to check the status of the service provisioning:\n     \n       l0 service get demo:shinkensvc \n     \n    When the service is ready, you will see the following output: SERVICE ID  SERVICE NAME  ENVIRONMENT  LOADBALANCER  DEPLOYS      SCALE (id)         shinkensvc    demo         shinkenlb     shinken:1    1/1", 
            "title": "Part 3: Create the service"
        }, 
        {
            "location": "/guides/shinken/#additional-steps-configuring-other-services", 
            "text": "In order to use Shinken health checks, you will need to modify the task definitions for your Layer0 applications. For more information, see the  Shinken Reference page .", 
            "title": "Additional steps: Configuring other services"
        }, 
        {
            "location": "/reference/cli/", 
            "text": "l0 command-line interface reference\n#\n\n\nGlobal options\n#\n\n\nThe \nl0\n application is designed to be used with one of several subcommands; these subcommands are detailed in the sections below. There are, however, some global parameters that you may specify when using \nl0\n.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0\n [\nglobalOptions\n] \ncommand\n \nsubcommand\n [\noptions\n] [\nparameters\n]\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--output {text|json}\n\n    \nSpecify the format of Layer0 outputs. By default, Layer0 outputs unformatted text; by issuing the \n--output json\n option, you can force \nl0\n to output JSON-formatted text.\n\n  \n\n  \n\n    \n--version\n\n    \nDisplay the version number of the \nl0\n application.\n\n  \n\n\n\n\n\n\nAdmin\n#\n\n\nThe \nadmin\n command is used to manage the Layer0 API server. This command is used with the following subcommands: \ndebug\n, \nsql\n, and \nversion\n.\n\n\nadmin debug\n#\n\n\nUse the \ndebug\n subcommand to view the running version of your Layer0 API server and CLI.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 admin debug\n\n  \n\n\n\n\nadmin sql\n#\n\n\nUse the \nsql\n subcommand to initialize the Layer0 API database.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 admin sql\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe \nsql\n subcommand is automatically executed during the Layer0 installation process; we recommend that you do not use this subcommand unless specifically directed to do so.\n\n  \n\n\n\n\nadmin version\n#\n\n\nUse the \nversion\n subcommand to display the current version of the Layer0 API.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 admin version\n\n  \n\n\n\n\n\n\nCertificate\n#\n\n\nIn order to use HTTPS ports in a Layer0 load balancer, you must create a certificate. You can use the Layer0 \ncertificate\n command to upload and manage these certificates. This command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n and \nlist\n.\n\n\ncertificate create\n#\n\n\nUse the \ncreate\n subcommand to upload a certificate into Layer0. Once you have uploaded the certificate, you can use it when configuring HTTPS ports on Layer0 load balancers.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 certificate create\n \ncertificateName publicKeyPath privateKeyPath\n [\nintermediateChainPath\n ]\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ncertificateName\n\n    \nA name for the certificate.\n\n  \n\n  \n\n    \npublicKeyPath\n\n    \nThe path to a public key associated with the certificate.\n\n  \n\n  \n\n    \nprivateKeyPath\n\n    \nThe path to the private key associated with the public key.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \nintermediateChainPath\n\n    \nThe path to an intermediate certificate authority.\n\n  \n\n\n\n\ncertificate delete\n#\n\n\nUse the \ndelete\n subcommand to delete a certificate that has already been uploaded to Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 certificate delete\n \ncertificateName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ncertificateName\n\n    \nThe name of the certificate you want to delete.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nYou will not be able to use the \ndelete\n subcommand if a load balancer that uses that certificate is currently running.\n\n  \n\n\n\n\ncertificate get\n#\n\n\nUse the \nget\n subcommand to display information about an existing certificate in Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 certificate get\n \ncertificateName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ncertificateName\n\n    \nThe name of the Layer0 certificate for which you want to view additional information.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \ncertificateName\n does not need to exactly match the name of an existing certificate. If multiple results are found that match the pattern you specified in \ncertificateName\n, then information about all matching certificates will be returned.\n\n  \n\n\n\n\ncertificate list\n#\n\n\nUse the \nlist\n subcommand to list all of the certificates used in an instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 certificate list\n\n  \n\n\n\n\n\n\nDeploy\n#\n\n\ndeploy create\n#\n\n\nUse the \ncreate\n subcommand to upload a Docker task definition into Layer0. This command is used with the following subcommands: \ncreate\n, \napply\n,  \ndelete\n, \nget\n and \nlist\n.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy create\n \ndockerPath\n \ndeployName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ndockerPath\n\n    \nThe path to the Docker task definition that you want to upload.\n\n  \n\n  \n\n    \ndeployName\n\n    \nA name for the deploy.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nIf \ndeployName\n exactly matches the name of an existing Layer0 deploy, then the version number of that deploy will increase by 1, and the task definition you specified will replace the task definition specified in the previous version.\n\n  \n \n\n  \n\n    \nIf you use Visual Studio to modify or create your Dockerrun file, you may see an \"Invalid Dockerrun.aws.json\" error. This error is caused by the default encoding used by Visual Studio. See the \n\"Common issues\" page\n for steps to resolve this issue.\n\n  \n\n\n\n\n\ndeploy delete\n#\n\n\nUse the \ndelete\n subcommand to delete a version of a Layer0 deploy.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy delete\n \ndeployID\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ndeployID\n\n    \nThe unique identifier of the version of the deploy that you want to delete. You can obtain a list of deployIDs for a given deploy by executing the following command: \nl0 deploy get\n \ndeployName\n\n  \n\n\n\n\ndeploy get\n#\n\n\nUse the \nget\n subcommand to view information about an existing Layer0 deploy.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy get\n \ndeployName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ndeployName\n\n    \nThe name of the Layer0 deploy for which you want to view additional information.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \ndeployName\n does not need to exactly match the name of an existing deploy. If multiple results are found that match the pattern you specified in \ndeployName\n, then information about all matching deploys will be returned.\n\n  \n\n\n\n\ndeploy list\n#\n\n\nUse the \nlist\n subcommand to view a list of deploys in your instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 deploy list\n\n  \n\n\n\n\n\n\nEnvironment\n#\n\n\nLayer0 environments allow you to isolate services and load balancers for specific applications.\nThe \nenvironment\n command is used to manage Layer0 environments. This command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nlist\n, and \nsetmincount\n.\n\n\nenvironment create\n#\n\n\nUse the \ncreate\n subcommand to create an additional Layer0 environment (\nenvironmentName\n).\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment create\n [--size] [--min-count] [--user-data] \nenvironmentName\n \n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nA name for the environment.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--size\n\n    \nThe size of the EC2 instances to create in your environment (default: m3.medium).\n\n  \n\n    \n\n    \n--min-count\n\n    \nThe minimum number of EC2 instances allowed in the environment's autoscaling group (default: 0).\n\n  \n\n  \n\n    \n--user-data\n\n    \nThe user data template to use for the environment's autoscaling group.\n\n  \n\n\n\n\nThe user data template can be used to add custom configuration to your Layer0 environment. \nLayer0 uses \nGo Templates\n to render user data. \nCurrently, two variables are passed into the template: \nECSEnvironmentID\n and \nS3Bucket\n.\nPlease review the \nECS Tutorial\n\nto better understand how to write a user data template, and use at your own risk! \nThe default Layer0 user data template is:\n\n\n#!/bin/bash\necho ECS_CLUSTER={{ .ECSEnvironmentID }} \n /etc/ecs/ecs.config\necho ECS_ENGINE_AUTH_TYPE=dockercfg \n /etc/ecs/ecs.config\nyum install -y aws-cli awslogs jq\naws s3 cp s3://{{ .S3Bucket }}/bootstrap/dockercfg dockercfg\ncfg=$(cat dockercfg)\necho ECS_ENGINE_AUTH_DATA=$cfg \n /etc/ecs/ecs.config\ndocker pull amazon/amazon-ecs-agent:latest\nstart ecs\n\n\n\n\nenvironment delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 environment.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment delete\n [--wait] \nenvironmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment that you want to delete.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--wait\n\n    \nWait until the deletion is complete before exiting.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n  \n\n\n\n\nenvironment get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 environment.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment get\n \nenvironmentName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment for which you want to view additional information.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \nenvironmentName\n does not need to exactly match the name of an existing environment. If multiple results are found that match the pattern you specified in \nenvironmentName\n, then information about all matching environments will be returned.\n\n  \n\n\n\n\nenvironment list\n#\n\n\nUse the \nlist\n subcommand to display a list of environments in your instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 environment list\n\n  \n\n\n\n\nenvironment setmincount\n#\n\n\nUse the \nsetmincount\n subcommand to set the minimum number of EC2 instances allowed the environment's autoscaling group.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 enviroment setmincount\n \nenvironmentName\n \ncount\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment that you want to delete.\n\n  \n\n  \n\n    \ncount\n\n    \nThe minimum number of instances allowed in the environment's autoscaling group.\n\n  \n\n\n\n\n\n\nJob\n#\n\n\nA Job is a long-running unit of work performed on behalf of the Layer0 API.\nJobs are executed as Layer0 tasks that run in the \napi\n Environment. \nThe \njob\n command is used with the following subcommands: \nlogs\n, \ndelete\n, \nget\n, and \nlist\n.\n\n\njob logs\n#\n\n\nUse the \nlogs\n subcommand to display the logs from a Layer0 job that is currently running.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job logs\n [--tail=\nN\n ] \njobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of the Layer0 job for which you want to view logs.\n\n  \n\n\n\n\njob logs\n#\n\n\nUse the \nlogs\n subcommand to display the logs from a Layer0 job that is currently running.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job logs\n [--tail=\nN\n ] \njobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of the Layer0 job for which you want to view logs.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--tail=\nN\n\n    \nDisplay only the last \nN\n lines of the log.\n\n  \n\n\n\n\njob delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing job.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job delete\n \njobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of the job that you want to delete.\n\n  \n\n\n\n\njob get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 job.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job get\n \njobName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \njobName\n\n    \nThe name of an existing Layer0 job.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \njobName\n does not need to exactly match the name of an existing job. If multiple results are found that match the pattern you specified in \njobName\n, then information about all matching jobs will be returned.\n\n  \n\n\n\n\njob list\n#\n\n\nUse the \nlist\n subcommand to display information about all of the existing jobs in an instance of Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 job list\n\n  \n\n\n\n\n\n\nLoadbalancer\n#\n\n\nA load balancer is a component of a Layer0 environment. Load balancers listen for traffic on certain ports, and then forward that traffic to Layer0 \nservices\n. The \nloadbalancer\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \naddport\n, \ndropport\n, \nget\n, and \nlist\n.\n\n\nloadbalancer create\n#\n\n\nUse the \ncreate\n subcommand to create a new load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer create\n [--port \nport\n --port \nport\n ...] [--certificate \ncertificateName\n] [--private] \nenvironmentName loadBalancerName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the existing Layer0 environment in which you want to create the load balancer.\n\n  \n\n  \n\n    \nloadBalancerName\n\n    \nA name for the load balancer.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--port \nhostPort:containerPort/protocol\n\n    \nThe port configuration for the load balancer. \nhostPort\n is the port on which the load balancer will listen for traffic; \ncontainerPort\n is the port that traffic will be forwarded to. You can specify multiple ports using \n--port xxx --port yyy\n. If this option is not specified, Layer0 will use the following configuration: 80:80/tcp\n\n  \n\n  \n\n    \n--certificate \ncertificateName\n\n    \nThe name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.\n\n  \n\n  \n\n    \n--private\n\n    \nWhen you use this option, the load balancer will only be accessible from within the Layer0 environment.\n\n  \n\n\n\n\nloadbalancer delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer delete\n [--wait] \nloadBalancerName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nloadBalancerName\n\n    \nThe name of the load balancer that you want to delete.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--wait\n\n    \nWait until the deletion is complete before exiting.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nIn order to delete a load balancer that is already attached to a service, you must first delete the service that uses the load balancer.\n\n  \n\n  \n\n    \nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n  \n\n\n\n\nloadbalancer addport\n#\n\n\nUse the \naddport\n subcommand to add a new port configuration to an existing Layer0 load balancer.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer addport\n \nloadBalancerName hostPort:containerPort/protocol\n [--certificate \ncertificateName\n]\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nloadBalancerName\n\n    \nThe name of an existing Layer0 load balancer in which you want to add the port configuration.\n\n  \n\n  \n\n    \nhostPort\n\n    \nThe port that the load balancer will listen on.\n\n  \n\n  \n\n    \ncontainerPort\n\n    \nThe port that the load balancer will forward traffic to.\n\n  \n\n  \n\n    \nprotocol\n\n    \nThe protocol to use when forwarding traffic (acceptable values: tcp, ssl, http, and https).\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--certificate \ncertificateName\n\n    \nThe name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe port configuration you specify must not already be in use by the load balancer you specify.\n\n  \n\n\n\n\nloadbalancer dropport\n#\n\n\nUse the \ndropport\n subcommand to remove a port configuration from an existing Layer0 load balancer. \n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer dropport\n \nloadBalancerName\n \nhostPort\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nloadBalancerName\n\n    \nThe name of an existing Layer0 load balancer in which you want to remove the port configuration.\n\n  \n\n  \n\n    \nhostPort\n\n    \nThe host port to remove from the load balancer.\n\n  \n\n\n\n\nloadbalancer get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 load balancer. \n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer get\n \nenvironmentName:loadBalancerName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of an existing Layer0 environment.\n\n  \n\n  \n\n    \nloadBalancerName\n\n    \nThe name of an existing Layer0 load balancer.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \nloadBalancerName\n does not need to exactly match the name of an existing load balancer. If multiple results are found that match the pattern you specified in \nloadBalancerName\n, then information about all matching load balancers will be returned.\n\n  \n\n\n\n\nloadbalancer list\n#\n\n\nUse the \nlist\n subcommand to display information about all of the existing load balancers in an instance of Layer0. \n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 loadbalancer list\n\n  \n\n\n\n\n\n\nService\n#\n\n\nA service is a component of a Layer0 environment. The purpose of a service is to execute a Docker image specified in a \ndeploy\n. In order to create a service, you must first create an \nenvironment\n and a \ndeploy\n; in most cases, you should also create a \nload balancer\n before creating the service.\n\n\nThe \nservice\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nlist\n, \nlogs\n, and \nscale\n.\n\n\nservice create\n#\n\n\nUse the \ncreate\n subcommand to create a Layer0 service. \n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service create\n [--loadbalancer \nenvironmentName:loadBalancerName\n ] [--no-logs] \nenvironmentName serviceName deployName:deployVersion\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nserviceName\n\n    \nA name for the service that you are creating.\n\n  \n\n  \n\n    \nenvironmentName\n\n    \nThe name of an existing Layer0 environment.\n\n  \n\n  \n\n    \ndeployName\n\n    \nThe name of a Layer0 deploy that exists in the environment \nenvironmentName\n.\n\n  \n \n  \n\n    \ndeployVersion\n\n    \nThe version number of the Layer0 deploy that you want to deploy. If you do not specify a version number, the latest version of the deploy will be used.\n\n  \n \n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--loadbalancer \nenvironmentName:loadBalancerName\n\n    \nPlace the new service behind an existing load balancer named \nloadBalancerName\n in the environment named \nenvironmentName\n.\n\n  \n\n  \n\n    \n--no-logs\n\n    \nDisable cloudwatch logging for the service\n\n  \n\n\n\n\nservice update\n#\n\n\nUse the \nupdate\n subcommand to apply an existing Layer0 Deploy to an existing Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service update\n [--no-logs] \nenvironmentName:serviceName deployName:deployVersion\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment in which the service resides.\n\n  \n\n  \n\n    \nserviceName\n\n    \nThe name of an existing Layer0 service into which you want to apply the deploy.\n\n  \n\n  \n\n    \ndeployName\n\n    \nThe name of the Layer0 deploy that you want to apply to the service.\n\n  \n\n  \n\n    \ndeployVersion\n\n    \nThe version of the Layer0 deploy that you want to apply to the service. If you do not specify a version number, the latest version of the deploy will be applied.\n\n  \n\n  \n\n    \n--no-logs\n\n    \nDisable cloudwatch logging for the service\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nIf your service uses a load balancer, when you update the task definition for the service, the container name and container port that were specified when the service was created must remain the same in the task definition. In other words, if your service has a load balancer, you cannot apply any deploy you want to that service. If you are varying the container name or exposed ports, you must create a new service instead.\n\n  \n\n\n\n\n\nservice delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service delete\n [--wait] \nenvironmentName:serviceName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment that contains the service you want to delete.\n\n  \n\n  \n\n    \nserviceName\n\n    \nThe name of the Layer0 service that you want to delete.\n\n  \n \n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--wait\n\n    \nWait until the deletion is complete before exiting.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThis operation performs several tasks asynchronously. When run without the \n--wait\n option, this operation will most likely exit before all of these tasks are complete; when run with the \n--wait\n option, this operation will only exit once these tasks have completed.\n\n  \n\n\n\n\nservice get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 service.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service get\n \nenvironmentName:serviceName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of an existing Layer0 environment.\n\n  \n \n  \n\n    \nserviceName\n\n    \nThe name of an existing Layer0 service.\n\n  \n \n\n\n\nservice list\n#\n\n\nUse the \nlist\n subcommand to list all of the existing services in your Layer0 instance.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service list\n\n  \n\n\n\n\nservice logs\n#\n\n\nUse the \nlogs\n subcommand to display the logs from a Layer0 service that is currently running.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service logs\n [--tail=\nN\n ] \nserviceName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nserviceName\n\n    \nThe name of the Layer0 service for which you want to view logs.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--tail=\nN\n\n    \nDisplay only the last \nN\n lines of the log.\n\n  \n\n\n\n\nservice scale\n#\n\n\nUse the \nscale\n subcommand to specify how many copies of an existing Layer0 service should run.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 service scale\n \nenvironmentName:serviceName N\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the Layer0 environment that contains the service that you want to scale.\n\n  \n\n  \n\n    \nserviceName\n\n    \nThe name of the Layer0 service that you want to scale up.\n\n  \n\n  \n\n    \nN\n\n    \nThe number of copies of the specified service that should be run.\n\n  \n\n\n\n\n\n\nTask\n#\n\n\nA Layer0 task is a component of an environment. A task executes the contents of a Docker image, as specified in a deploy. A task differs from a service in that a task does not restart after exiting. Additionally, ports are not exposed when using a task.\n\n\nThe \ntask\n command is used with the following subcommands: \ncreate\n, \ndelete\n, \nget\n, \nlist\n, and \nlogs\n.\n\n\ntask create\n#\n\n\nUse the \ncreate\n subcommand to create a Layer0 task.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task create\n [--no-logs] [--copies \ncopies\n] \nenvironmentName taskName deployName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nenvironmentName\n\n    \nThe name of the existing Layer0 environment in which you want to create the task.\n\n  \n\n  \n\n    \ntaskName\n\n    \nA name for the task.\n\n  \n\n  \n\n    \ndeployName\n\n    \nThe name of an existing Layer0 deploy that the task should use.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--copies\n\n    \nThe number of copies of the task to run (default: 1)\n\n  \n\n  \n\n    \n--no-logs\n\n    \nDisable cloudwatch logging for the service\n\n  \n\n\n\n\ntask delete\n#\n\n\nUse the \ndelete\n subcommand to delete an existing Layer0 task.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task delete\n [\nenvironmentName\n:]\ntaskName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ntaskName\n\n    \nThe name of the Layer0 task that you want to delete.\n\n  \n\n\n\n\nOptional parameters\n#\n\n\n\n  \n\n    \n[\nenvironmentName\n:]\n\n    \nThe name of the Layer0 environment that contains the task. This parameter is only necessary if multiple environments contain tasks with exactly the same name.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nUntil the record has been purged, the API may indicate that the task is still running. Task records are typically purged within an hour.\n\n  \n\n\n\n\ntask get\n#\n\n\nUse the \nget\n subcommand to display information about an existing Layer0 task (\ntaskName\n).\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task get\n [\nenvironmentName\n:]\ntaskName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ntaskName\n\n    \nThe name of a Layer0 task for which you want to see information.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \ntaskName\n does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in \ntaskName\n, then information about all matching tasks will be returned.\n\n  \n\n\n\n\ntask list\n#\n\n\nUse the \ntask\n subcommand to display a list of running tasks in your Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task list\n\n  \n\n\n\n\ntask logs\n#\n\n\nUse the \nlogs\n subcommand to display logs for a running Layer0 task.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task logs\n [--tail=\nN\n ] \ntaskName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \ntaskName\n\n    \nThe name of an existing Layer0 task.\n\n  \n\n\n\n\nAdditional information\n#\n\n\n\n  \n\n    \nThe value of \ntaskName\n does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in \ntaskName\n, then information about all matching tasks will be returned.\n\n  \n\n\n\n\ntask list\n#\n\n\nUse the \ntask\n subcommand to display a list of running tasks in your Layer0.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0 task list\n\n  \n\n\n\n=======", 
            "title": "Layer0 CLI"
        }, 
        {
            "location": "/reference/cli/#l0-command-line-interface-reference", 
            "text": "", 
            "title": "l0 command-line interface reference"
        }, 
        {
            "location": "/reference/cli/#global-options", 
            "text": "The  l0  application is designed to be used with one of several subcommands; these subcommands are detailed in the sections below. There are, however, some global parameters that you may specify when using  l0 .", 
            "title": "Global options"
        }, 
        {
            "location": "/reference/cli/#usage", 
            "text": "l0  [ globalOptions ]  command   subcommand  [ options ] [ parameters ]", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#optional-arguments", 
            "text": "--output {text|json} \n     Specify the format of Layer0 outputs. By default, Layer0 outputs unformatted text; by issuing the  --output json  option, you can force  l0  to output JSON-formatted text. \n   \n   \n     --version \n     Display the version number of the  l0  application.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#admin", 
            "text": "The  admin  command is used to manage the Layer0 API server. This command is used with the following subcommands:  debug ,  sql , and  version .", 
            "title": "Admin"
        }, 
        {
            "location": "/reference/cli/#admin-debug", 
            "text": "Use the  debug  subcommand to view the running version of your Layer0 API server and CLI.", 
            "title": "admin debug"
        }, 
        {
            "location": "/reference/cli/#usage_1", 
            "text": "l0 admin debug", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#admin-sql", 
            "text": "Use the  sql  subcommand to initialize the Layer0 API database.", 
            "title": "admin sql"
        }, 
        {
            "location": "/reference/cli/#usage_2", 
            "text": "l0 admin sql", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#additional-information", 
            "text": "The  sql  subcommand is automatically executed during the Layer0 installation process; we recommend that you do not use this subcommand unless specifically directed to do so.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#admin-version", 
            "text": "Use the  version  subcommand to display the current version of the Layer0 API.", 
            "title": "admin version"
        }, 
        {
            "location": "/reference/cli/#usage_3", 
            "text": "l0 admin version", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#certificate", 
            "text": "In order to use HTTPS ports in a Layer0 load balancer, you must create a certificate. You can use the Layer0  certificate  command to upload and manage these certificates. This command is used with the following subcommands:  create ,  delete ,  get  and  list .", 
            "title": "Certificate"
        }, 
        {
            "location": "/reference/cli/#certificate-create", 
            "text": "Use the  create  subcommand to upload a certificate into Layer0. Once you have uploaded the certificate, you can use it when configuring HTTPS ports on Layer0 load balancers.", 
            "title": "certificate create"
        }, 
        {
            "location": "/reference/cli/#usage_4", 
            "text": "l0 certificate create   certificateName publicKeyPath privateKeyPath  [ intermediateChainPath  ]", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters", 
            "text": "certificateName \n     A name for the certificate. \n   \n   \n     publicKeyPath \n     The path to a public key associated with the certificate. \n   \n   \n     privateKeyPath \n     The path to the private key associated with the public key.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_1", 
            "text": "intermediateChainPath \n     The path to an intermediate certificate authority.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#certificate-delete", 
            "text": "Use the  delete  subcommand to delete a certificate that has already been uploaded to Layer0.", 
            "title": "certificate delete"
        }, 
        {
            "location": "/reference/cli/#usage_5", 
            "text": "l0 certificate delete   certificateName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_1", 
            "text": "certificateName \n     The name of the certificate you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_1", 
            "text": "You will not be able to use the  delete  subcommand if a load balancer that uses that certificate is currently running.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#certificate-get", 
            "text": "Use the  get  subcommand to display information about an existing certificate in Layer0.", 
            "title": "certificate get"
        }, 
        {
            "location": "/reference/cli/#usage_6", 
            "text": "l0 certificate get   certificateName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_2", 
            "text": "certificateName \n     The name of the Layer0 certificate for which you want to view additional information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_2", 
            "text": "The value of  certificateName  does not need to exactly match the name of an existing certificate. If multiple results are found that match the pattern you specified in  certificateName , then information about all matching certificates will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#certificate-list", 
            "text": "Use the  list  subcommand to list all of the certificates used in an instance of Layer0.", 
            "title": "certificate list"
        }, 
        {
            "location": "/reference/cli/#usage_7", 
            "text": "l0 certificate list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#deploy", 
            "text": "", 
            "title": "Deploy"
        }, 
        {
            "location": "/reference/cli/#deploy-create", 
            "text": "Use the  create  subcommand to upload a Docker task definition into Layer0. This command is used with the following subcommands:  create ,  apply ,   delete ,  get  and  list .", 
            "title": "deploy create"
        }, 
        {
            "location": "/reference/cli/#usage_8", 
            "text": "l0 deploy create   dockerPath   deployName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_3", 
            "text": "dockerPath \n     The path to the Docker task definition that you want to upload. \n   \n   \n     deployName \n     A name for the deploy.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_3", 
            "text": "If  deployName  exactly matches the name of an existing Layer0 deploy, then the version number of that deploy will increase by 1, and the task definition you specified will replace the task definition specified in the previous version. \n     \n   \n     If you use Visual Studio to modify or create your Dockerrun file, you may see an \"Invalid Dockerrun.aws.json\" error. This error is caused by the default encoding used by Visual Studio. See the  \"Common issues\" page  for steps to resolve this issue.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#deploy-delete", 
            "text": "Use the  delete  subcommand to delete a version of a Layer0 deploy.", 
            "title": "deploy delete"
        }, 
        {
            "location": "/reference/cli/#usage_9", 
            "text": "l0 deploy delete   deployID", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_4", 
            "text": "deployID \n     The unique identifier of the version of the deploy that you want to delete. You can obtain a list of deployIDs for a given deploy by executing the following command:  l0 deploy get   deployName", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#deploy-get", 
            "text": "Use the  get  subcommand to view information about an existing Layer0 deploy.", 
            "title": "deploy get"
        }, 
        {
            "location": "/reference/cli/#usage_10", 
            "text": "l0 deploy get   deployName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_5", 
            "text": "deployName \n     The name of the Layer0 deploy for which you want to view additional information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_4", 
            "text": "The value of  deployName  does not need to exactly match the name of an existing deploy. If multiple results are found that match the pattern you specified in  deployName , then information about all matching deploys will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#deploy-list", 
            "text": "Use the  list  subcommand to view a list of deploys in your instance of Layer0.", 
            "title": "deploy list"
        }, 
        {
            "location": "/reference/cli/#usage_11", 
            "text": "l0 deploy list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#environment", 
            "text": "Layer0 environments allow you to isolate services and load balancers for specific applications.\nThe  environment  command is used to manage Layer0 environments. This command is used with the following subcommands:  create ,  delete ,  get ,  list , and  setmincount .", 
            "title": "Environment"
        }, 
        {
            "location": "/reference/cli/#environment-create", 
            "text": "Use the  create  subcommand to create an additional Layer0 environment ( environmentName ).", 
            "title": "environment create"
        }, 
        {
            "location": "/reference/cli/#usage_12", 
            "text": "l0 environment create  [--size] [--min-count] [--user-data]  environmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_6", 
            "text": "environmentName \n     A name for the environment.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_2", 
            "text": "--size \n     The size of the EC2 instances to create in your environment (default: m3.medium). \n   \n     \n     --min-count \n     The minimum number of EC2 instances allowed in the environment's autoscaling group (default: 0). \n   \n   \n     --user-data \n     The user data template to use for the environment's autoscaling group. \n     The user data template can be used to add custom configuration to your Layer0 environment. \nLayer0 uses  Go Templates  to render user data. \nCurrently, two variables are passed into the template:  ECSEnvironmentID  and  S3Bucket .\nPlease review the  ECS Tutorial \nto better understand how to write a user data template, and use at your own risk! \nThe default Layer0 user data template is:  #!/bin/bash\necho ECS_CLUSTER={{ .ECSEnvironmentID }}   /etc/ecs/ecs.config\necho ECS_ENGINE_AUTH_TYPE=dockercfg   /etc/ecs/ecs.config\nyum install -y aws-cli awslogs jq\naws s3 cp s3://{{ .S3Bucket }}/bootstrap/dockercfg dockercfg\ncfg=$(cat dockercfg)\necho ECS_ENGINE_AUTH_DATA=$cfg   /etc/ecs/ecs.config\ndocker pull amazon/amazon-ecs-agent:latest\nstart ecs", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#environment-delete", 
            "text": "Use the  delete  subcommand to delete an existing Layer0 environment.", 
            "title": "environment delete"
        }, 
        {
            "location": "/reference/cli/#usage_13", 
            "text": "l0 environment delete  [--wait]  environmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_7", 
            "text": "environmentName \n     The name of the Layer0 environment that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_3", 
            "text": "--wait \n     Wait until the deletion is complete before exiting.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_5", 
            "text": "This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#environment-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 environment.", 
            "title": "environment get"
        }, 
        {
            "location": "/reference/cli/#usage_14", 
            "text": "l0 environment get   environmentName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_8", 
            "text": "environmentName \n     The name of the Layer0 environment for which you want to view additional information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_6", 
            "text": "The value of  environmentName  does not need to exactly match the name of an existing environment. If multiple results are found that match the pattern you specified in  environmentName , then information about all matching environments will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#environment-list", 
            "text": "Use the  list  subcommand to display a list of environments in your instance of Layer0.", 
            "title": "environment list"
        }, 
        {
            "location": "/reference/cli/#usage_15", 
            "text": "l0 environment list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#environment-setmincount", 
            "text": "Use the  setmincount  subcommand to set the minimum number of EC2 instances allowed the environment's autoscaling group.", 
            "title": "environment setmincount"
        }, 
        {
            "location": "/reference/cli/#usage_16", 
            "text": "l0 enviroment setmincount   environmentName   count", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_9", 
            "text": "environmentName \n     The name of the Layer0 environment that you want to delete. \n   \n   \n     count \n     The minimum number of instances allowed in the environment's autoscaling group.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#job", 
            "text": "A Job is a long-running unit of work performed on behalf of the Layer0 API.\nJobs are executed as Layer0 tasks that run in the  api  Environment. \nThe  job  command is used with the following subcommands:  logs ,  delete ,  get , and  list .", 
            "title": "Job"
        }, 
        {
            "location": "/reference/cli/#job-logs", 
            "text": "Use the  logs  subcommand to display the logs from a Layer0 job that is currently running.", 
            "title": "job logs"
        }, 
        {
            "location": "/reference/cli/#usage_17", 
            "text": "l0 job logs  [--tail= N  ]  jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_10", 
            "text": "jobName \n     The name of the Layer0 job for which you want to view logs.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#job-logs_1", 
            "text": "Use the  logs  subcommand to display the logs from a Layer0 job that is currently running.", 
            "title": "job logs"
        }, 
        {
            "location": "/reference/cli/#usage_18", 
            "text": "l0 job logs  [--tail= N  ]  jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_11", 
            "text": "jobName \n     The name of the Layer0 job for which you want to view logs.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_4", 
            "text": "--tail= N \n     Display only the last  N  lines of the log.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#job-delete", 
            "text": "Use the  delete  subcommand to delete an existing job.", 
            "title": "job delete"
        }, 
        {
            "location": "/reference/cli/#usage_19", 
            "text": "l0 job delete   jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_12", 
            "text": "jobName \n     The name of the job that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#job-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 job.", 
            "title": "job get"
        }, 
        {
            "location": "/reference/cli/#usage_20", 
            "text": "l0 job get   jobName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_13", 
            "text": "jobName \n     The name of an existing Layer0 job.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_7", 
            "text": "The value of  jobName  does not need to exactly match the name of an existing job. If multiple results are found that match the pattern you specified in  jobName , then information about all matching jobs will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#job-list", 
            "text": "Use the  list  subcommand to display information about all of the existing jobs in an instance of Layer0.", 
            "title": "job list"
        }, 
        {
            "location": "/reference/cli/#usage_21", 
            "text": "l0 job list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#loadbalancer", 
            "text": "A load balancer is a component of a Layer0 environment. Load balancers listen for traffic on certain ports, and then forward that traffic to Layer0  services . The  loadbalancer  command is used with the following subcommands:  create ,  delete ,  addport ,  dropport ,  get , and  list .", 
            "title": "Loadbalancer"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-create", 
            "text": "Use the  create  subcommand to create a new load balancer.", 
            "title": "loadbalancer create"
        }, 
        {
            "location": "/reference/cli/#usage_22", 
            "text": "l0 loadbalancer create  [--port  port  --port  port  ...] [--certificate  certificateName ] [--private]  environmentName loadBalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_14", 
            "text": "environmentName \n     The name of the existing Layer0 environment in which you want to create the load balancer. \n   \n   \n     loadBalancerName \n     A name for the load balancer.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_5", 
            "text": "--port  hostPort:containerPort/protocol \n     The port configuration for the load balancer.  hostPort  is the port on which the load balancer will listen for traffic;  containerPort  is the port that traffic will be forwarded to. You can specify multiple ports using  --port xxx --port yyy . If this option is not specified, Layer0 will use the following configuration: 80:80/tcp \n   \n   \n     --certificate  certificateName \n     The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration. \n   \n   \n     --private \n     When you use this option, the load balancer will only be accessible from within the Layer0 environment.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-delete", 
            "text": "Use the  delete  subcommand to delete an existing load balancer.", 
            "title": "loadbalancer delete"
        }, 
        {
            "location": "/reference/cli/#usage_23", 
            "text": "l0 loadbalancer delete  [--wait]  loadBalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_15", 
            "text": "loadBalancerName \n     The name of the load balancer that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_6", 
            "text": "--wait \n     Wait until the deletion is complete before exiting.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_8", 
            "text": "In order to delete a load balancer that is already attached to a service, you must first delete the service that uses the load balancer. \n   \n   \n     This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-addport", 
            "text": "Use the  addport  subcommand to add a new port configuration to an existing Layer0 load balancer.", 
            "title": "loadbalancer addport"
        }, 
        {
            "location": "/reference/cli/#usage_24", 
            "text": "l0 loadbalancer addport   loadBalancerName hostPort:containerPort/protocol  [--certificate  certificateName ]", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_16", 
            "text": "loadBalancerName \n     The name of an existing Layer0 load balancer in which you want to add the port configuration. \n   \n   \n     hostPort \n     The port that the load balancer will listen on. \n   \n   \n     containerPort \n     The port that the load balancer will forward traffic to. \n   \n   \n     protocol \n     The protocol to use when forwarding traffic (acceptable values: tcp, ssl, http, and https).", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_7", 
            "text": "--certificate  certificateName \n     The name of an existing Layer0 certificate. You must include this option if you are using an HTTPS port configuration.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_9", 
            "text": "The port configuration you specify must not already be in use by the load balancer you specify.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-dropport", 
            "text": "Use the  dropport  subcommand to remove a port configuration from an existing Layer0 load balancer.", 
            "title": "loadbalancer dropport"
        }, 
        {
            "location": "/reference/cli/#usage_25", 
            "text": "l0 loadbalancer dropport   loadBalancerName   hostPort", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_17", 
            "text": "loadBalancerName \n     The name of an existing Layer0 load balancer in which you want to remove the port configuration. \n   \n   \n     hostPort \n     The host port to remove from the load balancer.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 load balancer.", 
            "title": "loadbalancer get"
        }, 
        {
            "location": "/reference/cli/#usage_26", 
            "text": "l0 loadbalancer get   environmentName:loadBalancerName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_18", 
            "text": "environmentName \n     The name of an existing Layer0 environment. \n   \n   \n     loadBalancerName \n     The name of an existing Layer0 load balancer.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_10", 
            "text": "The value of  loadBalancerName  does not need to exactly match the name of an existing load balancer. If multiple results are found that match the pattern you specified in  loadBalancerName , then information about all matching load balancers will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#loadbalancer-list", 
            "text": "Use the  list  subcommand to display information about all of the existing load balancers in an instance of Layer0.", 
            "title": "loadbalancer list"
        }, 
        {
            "location": "/reference/cli/#usage_27", 
            "text": "l0 loadbalancer list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#service", 
            "text": "A service is a component of a Layer0 environment. The purpose of a service is to execute a Docker image specified in a  deploy . In order to create a service, you must first create an  environment  and a  deploy ; in most cases, you should also create a  load balancer  before creating the service.  The  service  command is used with the following subcommands:  create ,  delete ,  get ,  list ,  logs , and  scale .", 
            "title": "Service"
        }, 
        {
            "location": "/reference/cli/#service-create", 
            "text": "Use the  create  subcommand to create a Layer0 service.", 
            "title": "service create"
        }, 
        {
            "location": "/reference/cli/#usage_28", 
            "text": "l0 service create  [--loadbalancer  environmentName:loadBalancerName  ] [--no-logs]  environmentName serviceName deployName:deployVersion", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_19", 
            "text": "serviceName \n     A name for the service that you are creating. \n   \n   \n     environmentName \n     The name of an existing Layer0 environment. \n   \n   \n     deployName \n     The name of a Layer0 deploy that exists in the environment  environmentName . \n    \n   \n     deployVersion \n     The version number of the Layer0 deploy that you want to deploy. If you do not specify a version number, the latest version of the deploy will be used.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_8", 
            "text": "--loadbalancer  environmentName:loadBalancerName \n     Place the new service behind an existing load balancer named  loadBalancerName  in the environment named  environmentName . \n   \n   \n     --no-logs \n     Disable cloudwatch logging for the service", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#service-update", 
            "text": "Use the  update  subcommand to apply an existing Layer0 Deploy to an existing Layer0 service.", 
            "title": "service update"
        }, 
        {
            "location": "/reference/cli/#usage_29", 
            "text": "l0 service update  [--no-logs]  environmentName:serviceName deployName:deployVersion", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_20", 
            "text": "environmentName \n     The name of the Layer0 environment in which the service resides. \n   \n   \n     serviceName \n     The name of an existing Layer0 service into which you want to apply the deploy. \n   \n   \n     deployName \n     The name of the Layer0 deploy that you want to apply to the service. \n   \n   \n     deployVersion \n     The version of the Layer0 deploy that you want to apply to the service. If you do not specify a version number, the latest version of the deploy will be applied. \n   \n   \n     --no-logs \n     Disable cloudwatch logging for the service", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_11", 
            "text": "If your service uses a load balancer, when you update the task definition for the service, the container name and container port that were specified when the service was created must remain the same in the task definition. In other words, if your service has a load balancer, you cannot apply any deploy you want to that service. If you are varying the container name or exposed ports, you must create a new service instead.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#service-delete", 
            "text": "Use the  delete  subcommand to delete an existing Layer0 service.", 
            "title": "service delete"
        }, 
        {
            "location": "/reference/cli/#usage_30", 
            "text": "l0 service delete  [--wait]  environmentName:serviceName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_21", 
            "text": "environmentName \n     The name of the Layer0 environment that contains the service you want to delete. \n   \n   \n     serviceName \n     The name of the Layer0 service that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_9", 
            "text": "--wait \n     Wait until the deletion is complete before exiting.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#additional-information_12", 
            "text": "This operation performs several tasks asynchronously. When run without the  --wait  option, this operation will most likely exit before all of these tasks are complete; when run with the  --wait  option, this operation will only exit once these tasks have completed.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#service-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 service.", 
            "title": "service get"
        }, 
        {
            "location": "/reference/cli/#usage_31", 
            "text": "l0 service get   environmentName:serviceName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_22", 
            "text": "environmentName \n     The name of an existing Layer0 environment. \n    \n   \n     serviceName \n     The name of an existing Layer0 service.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#service-list", 
            "text": "Use the  list  subcommand to list all of the existing services in your Layer0 instance.", 
            "title": "service list"
        }, 
        {
            "location": "/reference/cli/#usage_32", 
            "text": "l0 service list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#service-logs", 
            "text": "Use the  logs  subcommand to display the logs from a Layer0 service that is currently running.", 
            "title": "service logs"
        }, 
        {
            "location": "/reference/cli/#usage_33", 
            "text": "l0 service logs  [--tail= N  ]  serviceName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_23", 
            "text": "serviceName \n     The name of the Layer0 service for which you want to view logs.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_10", 
            "text": "--tail= N \n     Display only the last  N  lines of the log.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#service-scale", 
            "text": "Use the  scale  subcommand to specify how many copies of an existing Layer0 service should run.", 
            "title": "service scale"
        }, 
        {
            "location": "/reference/cli/#usage_34", 
            "text": "l0 service scale   environmentName:serviceName N", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_24", 
            "text": "environmentName \n     The name of the Layer0 environment that contains the service that you want to scale. \n   \n   \n     serviceName \n     The name of the Layer0 service that you want to scale up. \n   \n   \n     N \n     The number of copies of the specified service that should be run.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#task", 
            "text": "A Layer0 task is a component of an environment. A task executes the contents of a Docker image, as specified in a deploy. A task differs from a service in that a task does not restart after exiting. Additionally, ports are not exposed when using a task.  The  task  command is used with the following subcommands:  create ,  delete ,  get ,  list , and  logs .", 
            "title": "Task"
        }, 
        {
            "location": "/reference/cli/#task-create", 
            "text": "Use the  create  subcommand to create a Layer0 task.", 
            "title": "task create"
        }, 
        {
            "location": "/reference/cli/#usage_35", 
            "text": "l0 task create  [--no-logs] [--copies  copies ]  environmentName taskName deployName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_25", 
            "text": "environmentName \n     The name of the existing Layer0 environment in which you want to create the task. \n   \n   \n     taskName \n     A name for the task. \n   \n   \n     deployName \n     The name of an existing Layer0 deploy that the task should use.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-arguments_11", 
            "text": "--copies \n     The number of copies of the task to run (default: 1) \n   \n   \n     --no-logs \n     Disable cloudwatch logging for the service", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/cli/#task-delete", 
            "text": "Use the  delete  subcommand to delete an existing Layer0 task.", 
            "title": "task delete"
        }, 
        {
            "location": "/reference/cli/#usage_36", 
            "text": "l0 task delete  [ environmentName :] taskName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_26", 
            "text": "taskName \n     The name of the Layer0 task that you want to delete.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#optional-parameters", 
            "text": "[ environmentName :] \n     The name of the Layer0 environment that contains the task. This parameter is only necessary if multiple environments contain tasks with exactly the same name.", 
            "title": "Optional parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_13", 
            "text": "Until the record has been purged, the API may indicate that the task is still running. Task records are typically purged within an hour.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#task-get", 
            "text": "Use the  get  subcommand to display information about an existing Layer0 task ( taskName ).", 
            "title": "task get"
        }, 
        {
            "location": "/reference/cli/#usage_37", 
            "text": "l0 task get  [ environmentName :] taskName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_27", 
            "text": "taskName \n     The name of a Layer0 task for which you want to see information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_14", 
            "text": "The value of  taskName  does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in  taskName , then information about all matching tasks will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#task-list", 
            "text": "Use the  task  subcommand to display a list of running tasks in your Layer0.", 
            "title": "task list"
        }, 
        {
            "location": "/reference/cli/#usage_38", 
            "text": "l0 task list", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#task-logs", 
            "text": "Use the  logs  subcommand to display logs for a running Layer0 task.", 
            "title": "task logs"
        }, 
        {
            "location": "/reference/cli/#usage_39", 
            "text": "l0 task logs  [--tail= N  ]  taskName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/cli/#required-parameters_28", 
            "text": "taskName \n     The name of an existing Layer0 task.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/cli/#additional-information_15", 
            "text": "The value of  taskName  does not need to exactly match the name of an existing task. If multiple results are found that match the pattern you specified in  taskName , then information about all matching tasks will be returned.", 
            "title": "Additional information"
        }, 
        {
            "location": "/reference/cli/#task-list_1", 
            "text": "Use the  task  subcommand to display a list of running tasks in your Layer0.", 
            "title": "task list"
        }, 
        {
            "location": "/reference/cli/#usage_40", 
            "text": "l0 task list \n    \n=======", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/", 
            "text": "Layer0 Setup (l0-setup) command-line interface reference\n#\n\n\nThe \nl0-setup\n application is designed to be used with one of several commands; these commands are detailed in the sections below.\n\n\nGeneral Usage\n#\n\n\n\n  \n\n    \nl0-setup\n [--version] \ncommand\n [\noptions\n] [\nparameters\n]\n\n  \n\n\n\n\n\n\nApply\n#\n\n\nThe \napply\n command is used to create and update Layer0 instances.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup apply\n [--access_key=\nawsAccessKeyID\n] [--secret_key=\nawsSecretAccessKeyID\n] [--region=\nawsRegion\n] [--docker_token=\ndockerToken\n] [--vpc=\nvpcID\n] \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix in an existing AWS stack. To learn more about creating a stack, see the \ninstallation guide\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--access_key=\nawsAccessKeyID\n\n    \nThe Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance.\n\n  \n\n  \n\n    \n--secret_key=\nawsSecretAccessKeyID\n\n    \nThe Secret Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance.\n\n  \n\n  \n\n    \n--region=\nawsRegion\n\n    \nThe AWS region in which the Layer0 instance resides.\n\n  \n\n  \n\n    \n--docker_token=\ndockerToken\n\n    \nA valid d.ims.io Docker token.\n\n  \n\n  \n\n    \n--vpc=\nvpcID\n\n    \nThe ID of a VPC. If blank, \nl0-setup\n will create a new VPC.\n\n  \n\n\n\n\n\n\nBackup\n#\n\n\nThe \nbackup\n command is used to back up your Layer0 configuration files to an S3 bucket. This command is most often used when migrating between versions of Layer0. The \nbackup\n command also runs automatically every time you execute the \nl0-setup apply\n command.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup backup\n \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of the Layer0Prefix that you want to back up.\n\n  \n\n\n\n\n\n\nRestore\n#\n\n\nThe \nrestore\n command is used to restore Layer0 configuration files that were previously backed up to an S3 bucket using the \nbackup\n command.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup restore\n [--access_key=\nawsAccessKeyID\n] [--secret_key=\nawsSecretAccessKeyID\n] [--region=\nawsRegion\n] [--docker_token=\ndockerToken\n] \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of the Layer0Prefix that you want to restore.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--access_key=\nawsAccessKeyID\n\n    \nThe Access Key ID of an IAM user associated with the AWS stack in which the Layer0 instance was created.\n\n  \n\n  \n\n    \n--secret_key=\nawsSecretAccessKeyID\n\n    \nThe Secret Access Key ID of an IAM user associated with the AWS stack in which the Layer0 instance was created.\n\n  \n\n  \n\n    \n--region=\nawsRegion\n\n    \nThe AWS region in which the Layer0 instance resides.\n\n  \n\n  \n\n    \n--docker_token=\ndockerToken\n\n    \nA valid d.ims.io Docker token.\n\n  \n\n\n\n\n\n\nDestroy\n#\n\n\nThe \ndestroy\n command is used to delete a Layer0 configuration.\n\n\nBefore you can run the \ndestroy\n command, you must first delete any existing \nloadbalancers\n, \nservices\n and \nenvironments\n that exist in the Layer0 instance, using the \ndelete\n subcommands for each of those components.\n\n\n\n\nCaution\n\n\nDestroying a Layer0 instance cannot be undone; if you created backups of your Layer0 configuration using the \nbackup\n command, those backups will also be deleted when you run the \ndestroy\n command. The only way to re-deploy a destroyed instance is to use the procedures in the \ninstallation guide\n to rebuild it.\n\n\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup destroy\n \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix in an existing AWS stack.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--force=false\n\n    \nWhen specified, destroy confirmation prompts will not be shown.\n\n  \n\n\n\n\n\n\nEndpoint\n#\n\n\nThe \nendpoint\n command is used to look up the details of a Layer0 endpoint so that you can export them to your shell.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup endpoint\n [-iq] [-s \nsyntax\n] \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix for which you want to view endpoint information.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n-s, --syntax=\"bash\"\n\n    \nShow commands using the specified syntax (bash, powershell, cmd)\n\n  \n\n  \n\n    \n-i, --insecure=false\n\n    \nAllow incomplete SSL configuration. This option is not recommended for production use.\n\n  \n\n  \n\n    \n-q, --quiet=false\n\n    \nSilence CLI and API version mismatch warning messages\n\n  \n\n\n\n\n\n\nVPC\n#\n\n\nThe \nvpc\n command is used to look up details from a Virtual Private Cloud (VPC) instance.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup vpc\n [--access_key=\nawsAccessKeyID\n] [--secret_key=\nawsSecretAccessKeyID\n] [--region=\nawsRegion\n] [--docker_token=\ndockerToken\n] \nprefixName\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix for which you want to view VPC information.\n\n  \n\n\n\n\nOptional arguments\n#\n\n\n\n  \n\n    \n--access_key=\nawsAccessKeyID\n\n    \nThe Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance.\n\n  \n\n  \n\n    \n--secret_key=\nawsSecretAccessKeyID\n\n    \nThe Secret Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance.\n\n  \n\n  \n\n    \n--region=\nawsRegion\n\n    \nThe AWS region in which the Layer0 instance resides.\n\n  \n\n  \n\n    \n--docker_token=\ndockerToken\n\n    \nA valid d.ims.io Docker token.\n\n  \n\n\n\n\n\n\nTerraform\n#\n\n\nThe \nterraform\n command is used to issue terraform commands directly to a Layer0 instance. Some terraform commands enable functionality that is not a standard part of the \nl0-setup\n application.\n\n\nFor more information about the capabilities and syntax of terraform commands, see the \nTerraform Commands (CLI) Documentation\n.\n\n\nUsage\n#\n\n\n\n  \n\n    \nl0-setup terraform\n \nprefixName\n \nterraformArguments\n\n  \n\n\n\n\nRequired parameters\n#\n\n\n\n  \n\n    \nprefixName\n\n    \nThe name of a Layer0Prefix in which you want to execute terraform commands.\n\n  \n\n  \n\n    \nterraformArguments\n\n    \nThe terraform arguments to pass to Layer0. Arguments passed in this way must follow the syntax specified in the \nTerraform Commands (CLI) Documentation\n.\n\n  \n\n\n\n\n\n\nMigrate\n#\n\n\n\n\nNote\n\n\nThe \nmigrate\n command has been deprecated and should not be used. This command will be removed from future versions of l0-setup.", 
            "title": "Layer0 Setup CLI"
        }, 
        {
            "location": "/reference/setup-cli/#layer0-setup-l0-setup-command-line-interface-reference", 
            "text": "The  l0-setup  application is designed to be used with one of several commands; these commands are detailed in the sections below.", 
            "title": "Layer0 Setup (l0-setup) command-line interface reference"
        }, 
        {
            "location": "/reference/setup-cli/#general-usage", 
            "text": "l0-setup  [--version]  command  [ options ] [ parameters ]", 
            "title": "General Usage"
        }, 
        {
            "location": "/reference/setup-cli/#apply", 
            "text": "The  apply  command is used to create and update Layer0 instances.", 
            "title": "Apply"
        }, 
        {
            "location": "/reference/setup-cli/#usage", 
            "text": "l0-setup apply  [--access_key= awsAccessKeyID ] [--secret_key= awsSecretAccessKeyID ] [--region= awsRegion ] [--docker_token= dockerToken ] [--vpc= vpcID ]  prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters", 
            "text": "prefixName \n     The name of a Layer0Prefix in an existing AWS stack. To learn more about creating a stack, see the  installation guide", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments", 
            "text": "--access_key= awsAccessKeyID \n     The Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance. \n   \n   \n     --secret_key= awsSecretAccessKeyID \n     The Secret Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance. \n   \n   \n     --region= awsRegion \n     The AWS region in which the Layer0 instance resides. \n   \n   \n     --docker_token= dockerToken \n     A valid d.ims.io Docker token. \n   \n   \n     --vpc= vpcID \n     The ID of a VPC. If blank,  l0-setup  will create a new VPC.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/setup-cli/#backup", 
            "text": "The  backup  command is used to back up your Layer0 configuration files to an S3 bucket. This command is most often used when migrating between versions of Layer0. The  backup  command also runs automatically every time you execute the  l0-setup apply  command.", 
            "title": "Backup"
        }, 
        {
            "location": "/reference/setup-cli/#usage_1", 
            "text": "l0-setup backup   prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_1", 
            "text": "prefixName \n     The name of the Layer0Prefix that you want to back up.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#restore", 
            "text": "The  restore  command is used to restore Layer0 configuration files that were previously backed up to an S3 bucket using the  backup  command.", 
            "title": "Restore"
        }, 
        {
            "location": "/reference/setup-cli/#usage_2", 
            "text": "l0-setup restore  [--access_key= awsAccessKeyID ] [--secret_key= awsSecretAccessKeyID ] [--region= awsRegion ] [--docker_token= dockerToken ]  prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_2", 
            "text": "prefixName \n     The name of the Layer0Prefix that you want to restore.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments_1", 
            "text": "--access_key= awsAccessKeyID \n     The Access Key ID of an IAM user associated with the AWS stack in which the Layer0 instance was created. \n   \n   \n     --secret_key= awsSecretAccessKeyID \n     The Secret Access Key ID of an IAM user associated with the AWS stack in which the Layer0 instance was created. \n   \n   \n     --region= awsRegion \n     The AWS region in which the Layer0 instance resides. \n   \n   \n     --docker_token= dockerToken \n     A valid d.ims.io Docker token.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/setup-cli/#destroy", 
            "text": "The  destroy  command is used to delete a Layer0 configuration.  Before you can run the  destroy  command, you must first delete any existing  loadbalancers ,  services  and  environments  that exist in the Layer0 instance, using the  delete  subcommands for each of those components.   Caution  Destroying a Layer0 instance cannot be undone; if you created backups of your Layer0 configuration using the  backup  command, those backups will also be deleted when you run the  destroy  command. The only way to re-deploy a destroyed instance is to use the procedures in the  installation guide  to rebuild it.", 
            "title": "Destroy"
        }, 
        {
            "location": "/reference/setup-cli/#usage_3", 
            "text": "l0-setup destroy   prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_3", 
            "text": "prefixName \n     The name of a Layer0Prefix in an existing AWS stack.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments_2", 
            "text": "--force=false \n     When specified, destroy confirmation prompts will not be shown.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/setup-cli/#endpoint", 
            "text": "The  endpoint  command is used to look up the details of a Layer0 endpoint so that you can export them to your shell.", 
            "title": "Endpoint"
        }, 
        {
            "location": "/reference/setup-cli/#usage_4", 
            "text": "l0-setup endpoint  [-iq] [-s  syntax ]  prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_4", 
            "text": "prefixName \n     The name of a Layer0Prefix for which you want to view endpoint information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments_3", 
            "text": "-s, --syntax=\"bash\" \n     Show commands using the specified syntax (bash, powershell, cmd) \n   \n   \n     -i, --insecure=false \n     Allow incomplete SSL configuration. This option is not recommended for production use. \n   \n   \n     -q, --quiet=false \n     Silence CLI and API version mismatch warning messages", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/setup-cli/#vpc", 
            "text": "The  vpc  command is used to look up details from a Virtual Private Cloud (VPC) instance.", 
            "title": "VPC"
        }, 
        {
            "location": "/reference/setup-cli/#usage_5", 
            "text": "l0-setup vpc  [--access_key= awsAccessKeyID ] [--secret_key= awsSecretAccessKeyID ] [--region= awsRegion ] [--docker_token= dockerToken ]  prefixName", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_5", 
            "text": "prefixName \n     The name of a Layer0Prefix for which you want to view VPC information.", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#optional-arguments_4", 
            "text": "--access_key= awsAccessKeyID \n     The Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance. \n   \n   \n     --secret_key= awsSecretAccessKeyID \n     The Secret Access Key ID of an IAM user associated with the AWS stack in which you are creating the Layer0 instance. \n   \n   \n     --region= awsRegion \n     The AWS region in which the Layer0 instance resides. \n   \n   \n     --docker_token= dockerToken \n     A valid d.ims.io Docker token.", 
            "title": "Optional arguments"
        }, 
        {
            "location": "/reference/setup-cli/#terraform", 
            "text": "The  terraform  command is used to issue terraform commands directly to a Layer0 instance. Some terraform commands enable functionality that is not a standard part of the  l0-setup  application.  For more information about the capabilities and syntax of terraform commands, see the  Terraform Commands (CLI) Documentation .", 
            "title": "Terraform"
        }, 
        {
            "location": "/reference/setup-cli/#usage_6", 
            "text": "l0-setup terraform   prefixName   terraformArguments", 
            "title": "Usage"
        }, 
        {
            "location": "/reference/setup-cli/#required-parameters_6", 
            "text": "prefixName \n     The name of a Layer0Prefix in which you want to execute terraform commands. \n   \n   \n     terraformArguments \n     The terraform arguments to pass to Layer0. Arguments passed in this way must follow the syntax specified in the  Terraform Commands (CLI) Documentation .", 
            "title": "Required parameters"
        }, 
        {
            "location": "/reference/setup-cli/#migrate", 
            "text": "Note  The  migrate  command has been deprecated and should not be used. This command will be removed from future versions of l0-setup.", 
            "title": "Migrate"
        }, 
        {
            "location": "/reference/updateservice/", 
            "text": "Updating a Layer0 service\n#\n\n\nThere are three methods of updating an existing Layer0 service. The first method is to update the existing Deploy to refer to a new Docker task definition. The second method is to create a new Service that uses the same Loadbalancer. The third method is to create both a new Loadbalancer and a new Service.\n\n\nThere are advantages and disadvantages to each of these methods. The following sections discuss the advantages and disadvantages of using each method, and include procedures for implementing each method.\n\n\nMethod 1: Refer to a new task definition\n#\n\n\nThis method of updating a Layer0 application is the easiest to implement, because you do not need to rescale the Service or modify the Loadbalancer. This method is completely transparent to all other components of the application, and using this method does not involve any downtime.\n\n\nThe disadvantage of using this method is that you cannot perform A/B testing of the old and new services, and you cannot control which traffic goes to the old service and which goes to the new one.\n\n\nTo replace a Deploy to refer to a new task definition:\n\n\n\n\nAt the command line, type the following to create a new Deploy: \nl0 deploy create [pathToTaskDefinition] [deployName]\nNote that if \n[deployName]\n already exists, this step will create a new version of that Deploy. \n\n\nType the following to update the existing Service: \nl0 service update [existingServiceName] [deployName]\nBy default, the Service you specify in this command will refer to the latest version of \n[deployName]\n, if multiple versions of the Deploy exist.\nNote\nIf you want to refer to a specific version of the Deploy, type the following command instead of the one shown above: \nl0 service update [serviceName] [deployName]:[deployVersion]\n\n\n\n\nMethod 2: Create a new Deploy and Service using the same Loadbalancer\n#\n\n\nThis method of updating a Layer0 application is also rather easy to implement. Like the method described in the previous section, this method is completely transparent to all other services and components of the application. This method also you allows you to re-scale the service if necessary, using the \nl0 service scale\n command. Finally, this method allows for indirect A/B testing of the application; you can change the scale of the application, and observe the success and failure rates.\n\n\nThe disadvantage of using this method is that you cannot control the routing of traffic between the old and new versions of the application.\n\n\nTo create a new Deploy and Service:\n\n\n\n\nAt the command line, type the following to create a new Deploy (or a new version of the Deploy, if \n[deployName]\n already exists):\n \nl0 deploy create [pathToTaskDefinition] [deployName]\n \n\n\nType the following command to create a new Service that refers to \n[deployName]\n behind an existing Loadbalancer named \n[loadbalancerName]\n:\n \nl0 service create --loadbalancer [loadbalancerName] [environmentName] [deployName]\n \n\n\nCheck to make sure that the new Service is working as expected. If it is, and you do not want to keep the old Service, type the following command to delete the old Service: \nl0 service delete [oldServiceName]\n\n\n\n\nMethod 3: Create a new Deploy, Loadbalancer and Service\n#\n\n\nThe final method of updating a Layer0 service is to create an entirely new Deploy, Loadbalancer and Service. This method gives you complete control over both the new and the old Service, and allows you to perform true A/B testing by routing traffic to individual Services. \n\n\nThe disadvantage of using this method is that you need to implement a method of routing traffic between the new and the old Loadbalancer. \n\n\nTo create a new Deploy, Loadbalancer and Service:\n\n\n\n\nAt the command line, type the following command to create a new Deploy:\nl0 deploy create [pathToTaskDefinition] [deployName]\n\n\nType the following command to create a new Loadbalancer:\n \nl0 loadbalancer create --port [portNumber] [environmentName] [loadbalancerName] [deployName]\nNote\nThe value of \n[loadbalancerName]\n in the above command must be unique.\n\n\nType the following command to create a new Service: \nl0 service create --loadbalancer [loadBalancerName] [environmentName] [serviceName] [deployName]\nNote\nThe value of \n[serviceName]\n in the above command  must be unique.\n\n\nImplement a method of routing traffic between the old and new Services. Options for routing traffic include \nHAProxy\n and \nConsul\n.", 
            "title": "Updating a service"
        }, 
        {
            "location": "/reference/updateservice/#updating-a-layer0-service", 
            "text": "There are three methods of updating an existing Layer0 service. The first method is to update the existing Deploy to refer to a new Docker task definition. The second method is to create a new Service that uses the same Loadbalancer. The third method is to create both a new Loadbalancer and a new Service.  There are advantages and disadvantages to each of these methods. The following sections discuss the advantages and disadvantages of using each method, and include procedures for implementing each method.", 
            "title": "Updating a Layer0 service"
        }, 
        {
            "location": "/reference/updateservice/#method-1-refer-to-a-new-task-definition", 
            "text": "This method of updating a Layer0 application is the easiest to implement, because you do not need to rescale the Service or modify the Loadbalancer. This method is completely transparent to all other components of the application, and using this method does not involve any downtime.  The disadvantage of using this method is that you cannot perform A/B testing of the old and new services, and you cannot control which traffic goes to the old service and which goes to the new one.  To replace a Deploy to refer to a new task definition:   At the command line, type the following to create a new Deploy:  l0 deploy create [pathToTaskDefinition] [deployName] Note that if  [deployName]  already exists, this step will create a new version of that Deploy.   Type the following to update the existing Service:  l0 service update [existingServiceName] [deployName] By default, the Service you specify in this command will refer to the latest version of  [deployName] , if multiple versions of the Deploy exist. Note If you want to refer to a specific version of the Deploy, type the following command instead of the one shown above:  l0 service update [serviceName] [deployName]:[deployVersion]", 
            "title": "Method 1: Refer to a new task definition"
        }, 
        {
            "location": "/reference/updateservice/#method-2-create-a-new-deploy-and-service-using-the-same-loadbalancer", 
            "text": "This method of updating a Layer0 application is also rather easy to implement. Like the method described in the previous section, this method is completely transparent to all other services and components of the application. This method also you allows you to re-scale the service if necessary, using the  l0 service scale  command. Finally, this method allows for indirect A/B testing of the application; you can change the scale of the application, and observe the success and failure rates.  The disadvantage of using this method is that you cannot control the routing of traffic between the old and new versions of the application.  To create a new Deploy and Service:   At the command line, type the following to create a new Deploy (or a new version of the Deploy, if  [deployName]  already exists):   l0 deploy create [pathToTaskDefinition] [deployName]    Type the following command to create a new Service that refers to  [deployName]  behind an existing Loadbalancer named  [loadbalancerName] :   l0 service create --loadbalancer [loadbalancerName] [environmentName] [deployName]    Check to make sure that the new Service is working as expected. If it is, and you do not want to keep the old Service, type the following command to delete the old Service:  l0 service delete [oldServiceName]", 
            "title": "Method 2: Create a new Deploy and Service using the same Loadbalancer"
        }, 
        {
            "location": "/reference/updateservice/#method-3-create-a-new-deploy-loadbalancer-and-service", 
            "text": "The final method of updating a Layer0 service is to create an entirely new Deploy, Loadbalancer and Service. This method gives you complete control over both the new and the old Service, and allows you to perform true A/B testing by routing traffic to individual Services.   The disadvantage of using this method is that you need to implement a method of routing traffic between the new and the old Loadbalancer.   To create a new Deploy, Loadbalancer and Service:   At the command line, type the following command to create a new Deploy: l0 deploy create [pathToTaskDefinition] [deployName]  Type the following command to create a new Loadbalancer:   l0 loadbalancer create --port [portNumber] [environmentName] [loadbalancerName] [deployName] Note The value of  [loadbalancerName]  in the above command must be unique.  Type the following command to create a new Service:  l0 service create --loadbalancer [loadBalancerName] [environmentName] [serviceName] [deployName] Note The value of  [serviceName]  in the above command  must be unique.  Implement a method of routing traffic between the old and new Services. Options for routing traffic include  HAProxy  and  Consul .", 
            "title": "Method 3: Create a new Deploy, Loadbalancer and Service"
        }, 
        {
            "location": "/reference/consul/", 
            "text": "Consul reference\n#\n\n\nConsul\n is an open-source tool for discovering and configuring services in your network architecture. Specifically, Consul provides the following features:\n\n\n\n\nDiscovery of services\n\n\nMonitoring of the health of services\n\n\nKey/value storage with a simple HTTP API\n\n\n\n\nConsul Agent\n#\n\n\nThe \nConsul Agent\n exposes a DNS API for easy consumption of data generated by \nRegistrator\n. The Consul Agent can run either in server or client mode.\n\n\nWhen run as a Layer0 Service, the Consul Agent runs in server mode. To ensure the integrity of your data, the service in which you run consul should be scaled to size 3 or greater. A group of several consul deployments is known as a \"\ncluster\n.\"\n\n\nOther Layer0 Services that use Consul will run the Consul Agent in client mode, alongside their application containers.\nThe client is a very lightweight process that registers services, runs health checks, and forwards queries to servers.\n\n\nRegistrator\n#\n\n\nRegistrator\n is a tool that automatically registers and deregisters services into a Consul Cluster by inspecting Docker containers as they come online.\nContainer registration is based off of environment variables on the container.\nLayer0 Services that use Consul will run Registrator alongside their application containers.\n\n\nService Configuration\n#\n\n\nLayer0 Services that use Consul will need to add the \nRegistrator\n and \nConsul Agent\n definitions to the\n\ncontainerDefinitions\n section of your Deploys. You must also add the \nDocker Socket\n definition to the \nvolumes\n section of your Deploys.\n\n\nFor an example of a Deploy that uses Consul, see the \nGuestbook with Consul\n guide.\n\n\n\n\nRegistrator Container Definition\n#\n\n\n{\n    \nname\n: \nregistrator\n,\n    \nimage\n: \ngliderlabs/registrator:master\n,\n    \nessential\n: true,\n    \nlinks\n: [\nconsul-agent\n],\n    \nentrypoint\n: [\n/bin/sh\n, \n-c\n],\n    \ncommand\n: [\n/bin/registrator -retry-attempts=-1 -retry-interval=30000 -ip $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) consul://consul-agent:8500\n],\n    \nmemory\n: 128,\n    \nmountPoints\n: [\n        {\n            \nsourceVolume\n: \ndockersocket\n,\n            \ncontainerPath\n: \n/tmp/docker.sock\n\n        }\n    ]\n},\n\n\n\n\n\n\nConsul Agent Container Definition\n#\n\n\n\n\nWarning\n\n\n\n\nYou must replace \nurl\n with your Layer0 Consul Load Balancer's\n\n\n{\n    \nname\n: \nconsul-agent\n,\n    \nimage\n: \nprogrium/consul\n,\n    \nessential\n: true,\n    \nentrypoint\n: [\n/bin/bash\n, \n-c\n],\n    \ncommand\n: [\n/bin/start -advertise $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) -retry-join $EXTERNAL_URL -recursor $UPSTREAM_DNS -retry-interval 30s\n],\n    \nmemory\n: 128,\n    \nportMappings\n: [\n        {\n            \nhostPort\n: 8500,\n            \ncontainerPort\n: 8500\n        },\n        {\n            \nhostPort\n: 53,\n            \ncontainerPort\n: 53,\n            \nprotocol\n: \nudp\n\n        }\n    ],\n    \nenvironment\n: [\n        {\n            \nname\n: \nEXTERNAL_URL\n,\n            \nvalue\n: \nurl\n\n    },\n    {\n            \nname\n: \nUPSTREAM_DNS\n,\n            \nvalue\n: \n10.100.0.2\n\n        }\n    ]\n},\n\n\n\n\nEnvironment Variables\n#\n\n\n\n\nEXTERNAL_URL\n - URL of the consul cluster\n\n\nUPSTREAM_DNS\n - The DNS server consul-agent queries for DNS entries that it cannot resolve internally (e.g. google.com)\n\n\nThe default value for \nUPSTREAM_DNS\n assumes you're using the default Layer0 configuration, making your internal DNS endpoint 10.100.0.2.  If you are a using a non standard configuration (e.g. installing Layer0 in an existing VPC with a CIDR other than \n10.100.0.0/16\n) please modify this variable accordingly.\n\n\n\n\n\n\n\n\n\n\nDocker Socket Volume Definition\n#\n\n\nvolumes\n: [\n    {\n        \nname\n: \ndockersocket\n,\n        \nhost\n: {\n                \nsourcePath\n: \n/var/run/docker.sock\n\n        }\n    }\n],", 
            "title": "Consul"
        }, 
        {
            "location": "/reference/consul/#consul-reference", 
            "text": "Consul  is an open-source tool for discovering and configuring services in your network architecture. Specifically, Consul provides the following features:   Discovery of services  Monitoring of the health of services  Key/value storage with a simple HTTP API", 
            "title": "Consul reference"
        }, 
        {
            "location": "/reference/consul/#consul-agent", 
            "text": "The  Consul Agent  exposes a DNS API for easy consumption of data generated by  Registrator . The Consul Agent can run either in server or client mode.  When run as a Layer0 Service, the Consul Agent runs in server mode. To ensure the integrity of your data, the service in which you run consul should be scaled to size 3 or greater. A group of several consul deployments is known as a \" cluster .\"  Other Layer0 Services that use Consul will run the Consul Agent in client mode, alongside their application containers.\nThe client is a very lightweight process that registers services, runs health checks, and forwards queries to servers.", 
            "title": "Consul Agent"
        }, 
        {
            "location": "/reference/consul/#registrator", 
            "text": "Registrator  is a tool that automatically registers and deregisters services into a Consul Cluster by inspecting Docker containers as they come online.\nContainer registration is based off of environment variables on the container.\nLayer0 Services that use Consul will run Registrator alongside their application containers.", 
            "title": "Registrator"
        }, 
        {
            "location": "/reference/consul/#service-configuration", 
            "text": "Layer0 Services that use Consul will need to add the  Registrator  and  Consul Agent  definitions to the containerDefinitions  section of your Deploys. You must also add the  Docker Socket  definition to the  volumes  section of your Deploys.  For an example of a Deploy that uses Consul, see the  Guestbook with Consul  guide.", 
            "title": "Service Configuration"
        }, 
        {
            "location": "/reference/consul/#registrator-container-definition", 
            "text": "{\n     name :  registrator ,\n     image :  gliderlabs/registrator:master ,\n     essential : true,\n     links : [ consul-agent ],\n     entrypoint : [ /bin/sh ,  -c ],\n     command : [ /bin/registrator -retry-attempts=-1 -retry-interval=30000 -ip $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) consul://consul-agent:8500 ],\n     memory : 128,\n     mountPoints : [\n        {\n             sourceVolume :  dockersocket ,\n             containerPath :  /tmp/docker.sock \n        }\n    ]\n},", 
            "title": "Registrator Container Definition"
        }, 
        {
            "location": "/reference/consul/#consul-agent-container-definition", 
            "text": "Warning   You must replace  url  with your Layer0 Consul Load Balancer's  {\n     name :  consul-agent ,\n     image :  progrium/consul ,\n     essential : true,\n     entrypoint : [ /bin/bash ,  -c ],\n     command : [ /bin/start -advertise $(wget http://169.254.169.254/latest/meta-data/local-ipv4 -q -O -) -retry-join $EXTERNAL_URL -recursor $UPSTREAM_DNS -retry-interval 30s ],\n     memory : 128,\n     portMappings : [\n        {\n             hostPort : 8500,\n             containerPort : 8500\n        },\n        {\n             hostPort : 53,\n             containerPort : 53,\n             protocol :  udp \n        }\n    ],\n     environment : [\n        {\n             name :  EXTERNAL_URL ,\n             value :  url \n    },\n    {\n             name :  UPSTREAM_DNS ,\n             value :  10.100.0.2 \n        }\n    ]\n},", 
            "title": "Consul Agent Container Definition"
        }, 
        {
            "location": "/reference/consul/#environment-variables", 
            "text": "EXTERNAL_URL  - URL of the consul cluster  UPSTREAM_DNS  - The DNS server consul-agent queries for DNS entries that it cannot resolve internally (e.g. google.com)  The default value for  UPSTREAM_DNS  assumes you're using the default Layer0 configuration, making your internal DNS endpoint 10.100.0.2.  If you are a using a non standard configuration (e.g. installing Layer0 in an existing VPC with a CIDR other than  10.100.0.0/16 ) please modify this variable accordingly.", 
            "title": "Environment Variables"
        }, 
        {
            "location": "/reference/consul/#docker-socket-volume-definition", 
            "text": "volumes : [\n    {\n         name :  dockersocket ,\n         host : {\n                 sourcePath :  /var/run/docker.sock \n        }\n    }\n],", 
            "title": "Docker Socket Volume Definition"
        }, 
        {
            "location": "/reference/shinken/", 
            "text": "Shinken\n#\n\n\nShinken\n is an open source monitoring framework for software applications.\nXFRA provides \nLayer1-Shinken\n,\na containerized application that is optimized to integrate into your \nConsul-enabled\n Layer0 Environment.\n\n\nLayer1-Shinken dynamically detects:\n\n\n\n\nExternal URLs to query for status\n\n\nLayer0 Services registered within Consul that have health checks configured\n\n\nAlert notifications are delivered to your team via Slack\n\n\n\n\nConsul Health Checks\n#\n\n\n\n\nA \nConsul health check\n is configurable on each published port in your service by setting environment variables. \nConsul uses these health checks internally to determine the healthy members of a service cluster.\n\n\nThe environment variables are detected by \nRegistrator\n, which then automatically populates Consul when your container launches. \nYou can specify at most one check per port.\n\n\nConsul HTTP Health Check\n#\n\n\n\n\nHTTP Checks are the preferred method of monitoring service health in Consul.\n\n\nThe status of the service depends on the HTTP response code: \n\n\n\n\n\n\n\n\nStatus\n\n\nHealth\n\n\n\n\n\n\n\n\n\n\n200-299\n\n\nHealthy (Green)\n\n\n\n\n\n\n429\n\n\nWarning (Yellow)\n\n\n\n\n\n\nOther / No Response\n\n\nUnhealthy (Red)\n\n\n\n\n\n\n\n\nHTTP Checks are configured by setting environment variables in your Layer0 Service's task definition. \nServices can configure one health check per port that the container exposes. \nThe following environment variables are required for proper health check configuration:\n\n\nThe \"\n*\n\" portion of the environment variable must be replaced with an exposed port on the Service container.\n\n\n\n\nSERVICE_*_NAME\n: The name of your Service\n\n\nSERVICE_*_CHECK_HTTP\n: The route of your health check\n\n\nSERVICE_*_CHECK_INTERVAL\n: The interval between health checks \n\n\nSERVICE_*_CHECK_TIMEOUT\n: The timeout for the health check\n\n\n\n\nBelow is a snippet from a task defintion that configures a health check on port 80, against the \n/\n route, checking every 15 seconds with a 1 second timeout. \n\n\nenvironment\n: [\n    {\n        \nname\n: \nSERVICE_80_NAME\n,\n        \nvalue\n: \nsample\n\n    },\n    {\n        \nname\n: \nSERVICE_80_CHECK_HTTP\n,\n        \nvalue\n: \n/\n\n    },\n    {\n        \nname\n: \nSERVICE_80_CHECK_INTERVAL\n,\n        \nvalue\n: \n15s\n\n    },\n    {\n        \nname\n: \nSERVICE_80_CHECK_TIMEOUT\n,\n        \nvalue\n: \n1s\n\n    }\n]\n\n\n\n\nConsul TTL Health Check\n#\n\n\n\n\nOftentimes, you'll find your Service does not support HTTP requests, in which case implementing an HTTP endpoint just for health checks is a burden, though a viable option.\n\n\nAn alternative is using TTL (time to live) health checks.\nThese checks retain their last known state for a given TTL.\nThe state of the check must be updated periodically over Consul's HTTP interface.\nIf an external system fails to update the status within a given TTL, the check is set to the failed state.\nThis mechanism, conceptually similar to a dead man's switch, relies on the application to directly report its health.\nFor example, a healthy app can periodically PUT a status update to the HTTP endpoint; if the app fails, the TTL will expire and the health check enters a critical state.\n\n\nTo enable a TTL check on Service, simply include the \nSERVICE_CHECK_TTL\n environment variable with your timeout duration.\nFor example:\n\n\nenvironment\n: [\n    {\n        \nname\n: \nSERVICE_6379_NAME\n,\n        \nvalue\n: \ndb\n\n    },\n    {\n        \nname\n: \nSERVICE_6379_CHECK_TTL\n,\n        \nvalue\n: \n30s\n\n    }\n]\n\n\n\n\nThis may sound burndensome as well, but can be quite lightweight. \nXFRA provides a simple example of doing this to provide a health check on Redis \nhere\n.\n\n\nExternal Health Checks\n#\n\n\n\n\nLayer1-Shinken allows users to configure any number of external HTTP checks.\nYou can use this to monitor systems not managed by Layer0 from your Layer1-Shinken instance.\n\n\nExternal checks simply report on the state of an HTTP(s) URL.\nCheck health does not affect the availability of your Service, as in the case of \nConsul Health Checks\n, which automatically removes services with failing checks.\n\n\nTo determine the health of a URL, external checks look at the HTTP Status Code returned by the URL:\n\n\n\n\n\n\n\n\nStatus\n\n\nHealth\n\n\n\n\n\n\n\n\n\n\n200-299\n\n\nHealthy (Green)\n\n\n\n\n\n\n429\n\n\nWarning (Yellow)\n\n\n\n\n\n\nOther / No Response\n\n\nUnhealthy (Red)\n\n\n\n\n\n\n\n\nWhen using external health checks, it is recommended you build a variety of health URLs into your application to monitor the various mission critical aspects that can fail.\nThis way, you and your team have a simple means of being notified if/when one of those health checks reports a failure.\n\n\nConfigure External Checks\n#\n\n\nXFRA provides a Layer0 Service to manage and configure your external checks.\nPlease review its \nREADME.md\n for information on configuration options and usage.", 
            "title": "Shinken"
        }, 
        {
            "location": "/reference/shinken/#shinken", 
            "text": "Shinken  is an open source monitoring framework for software applications.\nXFRA provides  Layer1-Shinken ,\na containerized application that is optimized to integrate into your  Consul-enabled  Layer0 Environment.  Layer1-Shinken dynamically detects:   External URLs to query for status  Layer0 Services registered within Consul that have health checks configured  Alert notifications are delivered to your team via Slack", 
            "title": "Shinken"
        }, 
        {
            "location": "/reference/shinken/#consul-health-checks", 
            "text": "A  Consul health check  is configurable on each published port in your service by setting environment variables. \nConsul uses these health checks internally to determine the healthy members of a service cluster.  The environment variables are detected by  Registrator , which then automatically populates Consul when your container launches. \nYou can specify at most one check per port.", 
            "title": "Consul Health Checks"
        }, 
        {
            "location": "/reference/shinken/#consul-http-health-check", 
            "text": "HTTP Checks are the preferred method of monitoring service health in Consul.  The status of the service depends on the HTTP response code:      Status  Health      200-299  Healthy (Green)    429  Warning (Yellow)    Other / No Response  Unhealthy (Red)     HTTP Checks are configured by setting environment variables in your Layer0 Service's task definition. \nServices can configure one health check per port that the container exposes. \nThe following environment variables are required for proper health check configuration:  The \" * \" portion of the environment variable must be replaced with an exposed port on the Service container.   SERVICE_*_NAME : The name of your Service  SERVICE_*_CHECK_HTTP : The route of your health check  SERVICE_*_CHECK_INTERVAL : The interval between health checks   SERVICE_*_CHECK_TIMEOUT : The timeout for the health check   Below is a snippet from a task defintion that configures a health check on port 80, against the  /  route, checking every 15 seconds with a 1 second timeout.   environment : [\n    {\n         name :  SERVICE_80_NAME ,\n         value :  sample \n    },\n    {\n         name :  SERVICE_80_CHECK_HTTP ,\n         value :  / \n    },\n    {\n         name :  SERVICE_80_CHECK_INTERVAL ,\n         value :  15s \n    },\n    {\n         name :  SERVICE_80_CHECK_TIMEOUT ,\n         value :  1s \n    }\n]", 
            "title": "Consul HTTP Health Check"
        }, 
        {
            "location": "/reference/shinken/#consul-ttl-health-check", 
            "text": "Oftentimes, you'll find your Service does not support HTTP requests, in which case implementing an HTTP endpoint just for health checks is a burden, though a viable option.  An alternative is using TTL (time to live) health checks.\nThese checks retain their last known state for a given TTL.\nThe state of the check must be updated periodically over Consul's HTTP interface.\nIf an external system fails to update the status within a given TTL, the check is set to the failed state.\nThis mechanism, conceptually similar to a dead man's switch, relies on the application to directly report its health.\nFor example, a healthy app can periodically PUT a status update to the HTTP endpoint; if the app fails, the TTL will expire and the health check enters a critical state.  To enable a TTL check on Service, simply include the  SERVICE_CHECK_TTL  environment variable with your timeout duration.\nFor example:  environment : [\n    {\n         name :  SERVICE_6379_NAME ,\n         value :  db \n    },\n    {\n         name :  SERVICE_6379_CHECK_TTL ,\n         value :  30s \n    }\n]  This may sound burndensome as well, but can be quite lightweight. \nXFRA provides a simple example of doing this to provide a health check on Redis  here .", 
            "title": "Consul TTL Health Check"
        }, 
        {
            "location": "/reference/shinken/#external-health-checks", 
            "text": "Layer1-Shinken allows users to configure any number of external HTTP checks.\nYou can use this to monitor systems not managed by Layer0 from your Layer1-Shinken instance.  External checks simply report on the state of an HTTP(s) URL.\nCheck health does not affect the availability of your Service, as in the case of  Consul Health Checks , which automatically removes services with failing checks.  To determine the health of a URL, external checks look at the HTTP Status Code returned by the URL:     Status  Health      200-299  Healthy (Green)    429  Warning (Yellow)    Other / No Response  Unhealthy (Red)     When using external health checks, it is recommended you build a variety of health URLs into your application to monitor the various mission critical aspects that can fail.\nThis way, you and your team have a simple means of being notified if/when one of those health checks reports a failure.", 
            "title": "External Health Checks"
        }, 
        {
            "location": "/reference/shinken/#configure-external-checks", 
            "text": "XFRA provides a Layer0 Service to manage and configure your external checks.\nPlease review its  README.md  for information on configuration options and usage.", 
            "title": "Configure External Checks"
        }, 
        {
            "location": "/reference/task_definition/", 
            "text": "Task Definitions\n#\n\n\nThis guide gives some overview into the composition of a task definition.\nFor more comprehensive documentation, we recommend taking a look at the official AWS docs:\n\n\n\n\nCreating a Task Definition\n\n\nTask Definition Parameters\n\n\n\n\nSample\n#\n\n\nThe following snippet contains the task definition for the \nGuestbook\n application\n\n\n{\n    \nAWSEBDockerrunVersion\n: 2,\n    \ncontainerDefinitions\n: [\n        {\n            \nname\n: \nl0-demo-guestbook\n,\n            \nimage\n: \nd.ims.io/xfra/l0-guestbook\n,\n            \nessential\n: true,\n            \nmemory\n: 128,\n            \nportMappings\n: [\n                {\n                    \nhostPort\n: 80,\n                    \ncontainerPort\n: 80\n                }\n            ],\n            \nenvironment\n: [\n                {\n                    \nname\n: \nSERVICE_80_NAME\n,\n                    \nvalue\n: \nl0-guestbook\n\n                }\n            ]\n        }\n    ]\n}\n\n\n\n\n\n\nName\n The name of the container\n\n\n\n\n\n\nWarning\n\n\n\n\nIf you wish to update your task definition, the container names \nmust\n remain the same.\nIf any container names are changed or removed in an updated task definition,\nECS will not know how the existing container(s) should be mapped over and you will not be able to deploy the updated task definition.\nIf you encounter a scenario where you must change or remove a container's name in a task definition, we recommend re-creating the Layer0 Deploy and Service.\n\n\n\n\nImage\n The Docker image used to build the container. The image format is \nurl/image:tag\n\n\nThe \nurl\n specifies which Docker Repo to pull the image from (e.g. \nd.ims.io\n).\nIf \nurl\n is not specified, \nDocker Hub\n is used\n\n\nThe \nimage\n specifies the name of the image to grab\n\n\nThe \ntag\n specifies which version of image to grab.\nIf \ntag\n is not specified, \n:latest\n is used\n\n\n\n\n\n\nEssential\n If set to \ntrue\n, all other containers in the task definition will be stopped if that container fails or stops for any reason.\nOtherwise, the container's failure will not affect the rest of the containers in the task definition.\n\n\nMemory\n The number of MiB of memory to reserve for the container.\nIf your container attempts to exceed the memory allocated here, the container is killed\n\n\nPortMappings\n A list of hostPort, containerPort mappings for the container\n\n\nHostPort\n The port number on the host instance reserved for your container.\nIf your Layer0 Service is behind a Layer0 Load Balancer, this should map to an \ninstancePort\n on the Layer0 Load Balancer.\n\n\nContainerPort\n The port number the container should receive traffic on.\nAny traffic received from the instance's \nhostPort\n will be forwarded to the container on this port\n\n\n\n\n\n\nEnvironment\n A list of key/value pairs that will be available to the container as environment variables", 
            "title": "Task Definitions"
        }, 
        {
            "location": "/reference/task_definition/#task-definitions", 
            "text": "This guide gives some overview into the composition of a task definition.\nFor more comprehensive documentation, we recommend taking a look at the official AWS docs:   Creating a Task Definition  Task Definition Parameters", 
            "title": "Task Definitions"
        }, 
        {
            "location": "/reference/task_definition/#sample", 
            "text": "The following snippet contains the task definition for the  Guestbook  application  {\n     AWSEBDockerrunVersion : 2,\n     containerDefinitions : [\n        {\n             name :  l0-demo-guestbook ,\n             image :  d.ims.io/xfra/l0-guestbook ,\n             essential : true,\n             memory : 128,\n             portMappings : [\n                {\n                     hostPort : 80,\n                     containerPort : 80\n                }\n            ],\n             environment : [\n                {\n                     name :  SERVICE_80_NAME ,\n                     value :  l0-guestbook \n                }\n            ]\n        }\n    ]\n}   Name  The name of the container    Warning   If you wish to update your task definition, the container names  must  remain the same.\nIf any container names are changed or removed in an updated task definition,\nECS will not know how the existing container(s) should be mapped over and you will not be able to deploy the updated task definition.\nIf you encounter a scenario where you must change or remove a container's name in a task definition, we recommend re-creating the Layer0 Deploy and Service.   Image  The Docker image used to build the container. The image format is  url/image:tag  The  url  specifies which Docker Repo to pull the image from (e.g.  d.ims.io ).\nIf  url  is not specified,  Docker Hub  is used  The  image  specifies the name of the image to grab  The  tag  specifies which version of image to grab.\nIf  tag  is not specified,  :latest  is used    Essential  If set to  true , all other containers in the task definition will be stopped if that container fails or stops for any reason.\nOtherwise, the container's failure will not affect the rest of the containers in the task definition.  Memory  The number of MiB of memory to reserve for the container.\nIf your container attempts to exceed the memory allocated here, the container is killed  PortMappings  A list of hostPort, containerPort mappings for the container  HostPort  The port number on the host instance reserved for your container.\nIf your Layer0 Service is behind a Layer0 Load Balancer, this should map to an  instancePort  on the Layer0 Load Balancer.  ContainerPort  The port number the container should receive traffic on.\nAny traffic received from the instance's  hostPort  will be forwarded to the container on this port    Environment  A list of key/value pairs that will be available to the container as environment variables", 
            "title": "Sample"
        }, 
        {
            "location": "/reference/architecture/", 
            "text": "Layer0 Architecture\n#\n\n\nLayer0 is built on top of the following primary technologies:\n\n\n\n\nApplication Container: \nDocker\n\n\nCloud Provider: \nAmazon Web Services\n\n\nProvider Billing, Configuration: \nBring Your Own Ops (BYOO)\n\n\nContainer Management: \nAmazon EC2 Container Service (ECS)\n\n\nLoad Balancing: \nAmazon Elastic Load Balancing\n\n\nInfrastructure Configuration: Hashicorp \nTerraform\n\n\nIdentity Management: \nAuth0\n and IMS Active Directory", 
            "title": "Architecture"
        }, 
        {
            "location": "/reference/architecture/#layer0-architecture", 
            "text": "Layer0 is built on top of the following primary technologies:   Application Container:  Docker  Cloud Provider:  Amazon Web Services  Provider Billing, Configuration:  Bring Your Own Ops (BYOO)  Container Management:  Amazon EC2 Container Service (ECS)  Load Balancing:  Amazon Elastic Load Balancing  Infrastructure Configuration: Hashicorp  Terraform  Identity Management:  Auth0  and IMS Active Directory", 
            "title": "Layer0 Architecture"
        }, 
        {
            "location": "/reference/ecr/", 
            "text": "EC2 Container Registry\n#\n\n\nECR is an Amazon implementation of a docker registry.  It acts as a private registry in your AWS account, which can be accessed from any docker client, and Layer0.  Consider using ECR if you have stability issues with hosted docker registries, and do not wish to share your images publicly on \ndockerhub\n.\n\n\nSetup\n#\n\n\nWhen interacting with ECR, you will first need to create a repository and a login to interact from your development machine.\n\n\nRepository\n#\n\n\nEach repository needs to be created by an AWS api call.\n\n\n  \n aws ecr create-repository --repository-name myteam/myproject\n\n\n\n\nLogin\n#\n\n\nTo authenticate with the ECR service, Amazon provides the \nget-login\n command, which generates an authentication token, and returns a docker command to set it up\n\n\n  \n aws ecr get-login\n  # this command will return the following: (password is typically hundreds of characters)\n  docker login -u AWS -p password -e none https://aws_account_id.dkr.ecr.us-east-1.amazonaws.com\n\n\n\n\nExecute the provided docker command to store the login credentials\n\n\nAfterward creating the repository and local login credentials you may interact with images (and tags) under this path from a local docker client.\n\n\n  docker pull ${ecr-url}/myteam/myproject\n  docker push ${ecr-url}/myteam/myproject:custom-tag-1\n\n\n\n\nDeploy Example\n#\n\n\nHere we'll walk through using ECR when deploying to Layer0,  Using a very basic wait container.\n\n\nMake docker image\n#\n\n\nYour docker image can be built locally or pulled from dockerhub.  For this example, we made a service that waits and then exits (useful for triggering regular restarts).\n\n\n\n\nDockerfile\n\n\n\n\nFROM busybox\n\nENV SLEEP_TIME=60\n\nCMD sleep $SLEEP_TIME\n\n\n\n\nThen build the file, with the tag \nxfra/wait\n\n\n \n docker build -f Dockerfile.wait -t xfra/wait .\n\n\n\n\nUpload to ECR\n#\n\n\nAfter preparing a login and registry, tag the image with the remote url, and use \ndocker push\n\n\n  docker tag xfra/wait 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n  docker push 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n\n\n\n\n\n\nNote: your account id in this url will be different.\n\n\n\n\nCreate a deploy\n#\n\n\nTo run this image in Layer0, we create a dockerrun file, describing the instance and any additional variables\n\n\n\n\ntimeout.Dockerrun.aws.json\n\n\n\n\n{\n  \ncontainerDefinitions\n: [\n    {\n      \nname\n: \ntimeout\n,\n      \nimage\n: \n111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait:latest\n,\n      \nessential\n: true,\n      \nmemory\n: 10,\n      \nenvironment\n: [\n        { \nname\n: \nSLEEP_TIME\n, \nvalue\n: \n43200\n }\n      ]\n    }\n  ]\n}\n\n\n\n\nAnd create that in Layer0\n\n\n  l0 deploy create timeout.dockerrun.aws.json timeout\n\n\n\n\nDeploy\n#\n\n\nFinally, run that deploy as a service or a task. (the service will restart every 12 hours)\n\n\n  l0 service create demo timeoutsvc timeout:latest\n\n\n\n\nReferences\n#\n\n\n\n\nECR User Guide\n\n\ncreate-repository\n\n\nget-login", 
            "title": "ECR"
        }, 
        {
            "location": "/reference/ecr/#ec2-container-registry", 
            "text": "ECR is an Amazon implementation of a docker registry.  It acts as a private registry in your AWS account, which can be accessed from any docker client, and Layer0.  Consider using ECR if you have stability issues with hosted docker registries, and do not wish to share your images publicly on  dockerhub .", 
            "title": "EC2 Container Registry"
        }, 
        {
            "location": "/reference/ecr/#setup", 
            "text": "When interacting with ECR, you will first need to create a repository and a login to interact from your development machine.", 
            "title": "Setup"
        }, 
        {
            "location": "/reference/ecr/#repository", 
            "text": "Each repository needs to be created by an AWS api call.      aws ecr create-repository --repository-name myteam/myproject", 
            "title": "Repository"
        }, 
        {
            "location": "/reference/ecr/#login", 
            "text": "To authenticate with the ECR service, Amazon provides the  get-login  command, which generates an authentication token, and returns a docker command to set it up      aws ecr get-login\n  # this command will return the following: (password is typically hundreds of characters)\n  docker login -u AWS -p password -e none https://aws_account_id.dkr.ecr.us-east-1.amazonaws.com  Execute the provided docker command to store the login credentials  Afterward creating the repository and local login credentials you may interact with images (and tags) under this path from a local docker client.    docker pull ${ecr-url}/myteam/myproject\n  docker push ${ecr-url}/myteam/myproject:custom-tag-1", 
            "title": "Login"
        }, 
        {
            "location": "/reference/ecr/#deploy-example", 
            "text": "Here we'll walk through using ECR when deploying to Layer0,  Using a very basic wait container.", 
            "title": "Deploy Example"
        }, 
        {
            "location": "/reference/ecr/#make-docker-image", 
            "text": "Your docker image can be built locally or pulled from dockerhub.  For this example, we made a service that waits and then exits (useful for triggering regular restarts).   Dockerfile   FROM busybox\n\nENV SLEEP_TIME=60\n\nCMD sleep $SLEEP_TIME  Then build the file, with the tag  xfra/wait     docker build -f Dockerfile.wait -t xfra/wait .", 
            "title": "Make docker image"
        }, 
        {
            "location": "/reference/ecr/#upload-to-ecr", 
            "text": "After preparing a login and registry, tag the image with the remote url, and use  docker push    docker tag xfra/wait 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait\n  docker push 111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait   Note: your account id in this url will be different.", 
            "title": "Upload to ECR"
        }, 
        {
            "location": "/reference/ecr/#create-a-deploy", 
            "text": "To run this image in Layer0, we create a dockerrun file, describing the instance and any additional variables   timeout.Dockerrun.aws.json   {\n   containerDefinitions : [\n    {\n       name :  timeout ,\n       image :  111222333444.dkr.ecr.us-east-1.amazonaws.com/xfra-wait:latest ,\n       essential : true,\n       memory : 10,\n       environment : [\n        {  name :  SLEEP_TIME ,  value :  43200  }\n      ]\n    }\n  ]\n}  And create that in Layer0    l0 deploy create timeout.dockerrun.aws.json timeout", 
            "title": "Create a deploy"
        }, 
        {
            "location": "/reference/ecr/#deploy", 
            "text": "Finally, run that deploy as a service or a task. (the service will restart every 12 hours)    l0 service create demo timeoutsvc timeout:latest", 
            "title": "Deploy"
        }, 
        {
            "location": "/reference/ecr/#references", 
            "text": "ECR User Guide  create-repository  get-login", 
            "title": "References"
        }, 
        {
            "location": "/troubleshooting/commonissues/", 
            "text": "Common issues and their solutions\n#\n\n\n\"Connection refused\" error when executing Layer0 commands\n#\n\n\nWhen executing commands using the Layer0 CLI, you may see the following error message: \"Get http://localhost:9090/\ncommand\n/: dial tcp 127.0.0.1:9090: connection refused\", where \ncommand\n is the Layer0 command you are trying to execute.\n\n\nThis error indicates that your Layer0 environment variables have not been set for the current session. See the \n\"Configure environment variables\" section\n of the Layer0 installation guide for instructions for setting up your environment variables.\n\n\n\n\n\"Invalid Dockerrun.aws.json\" error when creating a deploy\n#\n\n\nByte Order Marks (BOM) in Dockerrun file\n#\n\n\nIf your Dockerrun.aws.json file contains a Byte Order Marker, you may receive an \"Invalid Dockerrun.aws.json\" error when creating a deploy. If you create or edit the Dockerrun file using Visual Studio, and you have not modified the file encoding settings in Visual Studio, you are likely to encounter this error.\n\n\nTo remove the BOM:\n\n\n\n\n\n\nAt the command line, type the following to remove the BOM:\n\n\n\n\n(Linux/OS X) \ntail -c +4\n \nDockerrunFile\n \n \nDockerrunFileNew\n\n\nReplace \nDockerrunFile\n with the path to your Dockerrun file, and \nDockerrunFileNew\n with a new name for the Dockerrun file without the BOM.\n\n\n\n\n\n\n\n\nAlternatively, you can use the \ndos2unix file converter\n to remove the BOM from your Dockerrun files. Dos2unix is available for Windows, Linux and Mac OS.\n\n\nTo remove the BOM using dos2unix:\n\n\n\n\n\n\nAt the command line, type the following:\n\n\n\n\ndos2unix --remove-bom -n\n \nDockerrunFile\n \nDockerrunFileNew\n\n\nReplace \nDockerrunFile\n with the path to your Dockerrun file, and \nDockerrunFileNew\n with a new name for the Dockerrun file without the BOM.\n\n\n\n\n\n\n\n\n\n\n\"AWS Error: the key pair '\n' does not exist (code 'ValidationError')\" with l0-setup\n#\n\n\nThis occurs when you pass a non-existent EC2 keypair to l0-setup. To fix this, follow the instructions for \ncreating an EC2 Key Pair\n.\n\n\n\n\nAfter you've created a new EC2 Key Pair, run the following command:\n\n\n  \nl0-setup plan\n \nprefix\n \n-var key_pair\n=\nkeypair", 
            "title": "Common Issues"
        }, 
        {
            "location": "/troubleshooting/commonissues/#common-issues-and-their-solutions", 
            "text": "", 
            "title": "Common issues and their solutions"
        }, 
        {
            "location": "/troubleshooting/commonissues/#connection-refused-error-when-executing-layer0-commands", 
            "text": "When executing commands using the Layer0 CLI, you may see the following error message: \"Get http://localhost:9090/ command /: dial tcp 127.0.0.1:9090: connection refused\", where  command  is the Layer0 command you are trying to execute.  This error indicates that your Layer0 environment variables have not been set for the current session. See the  \"Configure environment variables\" section  of the Layer0 installation guide for instructions for setting up your environment variables.", 
            "title": "\"Connection refused\" error when executing Layer0 commands"
        }, 
        {
            "location": "/troubleshooting/commonissues/#invalid-dockerrunawsjson-error-when-creating-a-deploy", 
            "text": "", 
            "title": "\"Invalid Dockerrun.aws.json\" error when creating a deploy"
        }, 
        {
            "location": "/troubleshooting/commonissues/#byte-order-marks-bom-in-dockerrun-file", 
            "text": "If your Dockerrun.aws.json file contains a Byte Order Marker, you may receive an \"Invalid Dockerrun.aws.json\" error when creating a deploy. If you create or edit the Dockerrun file using Visual Studio, and you have not modified the file encoding settings in Visual Studio, you are likely to encounter this error.  To remove the BOM:    At the command line, type the following to remove the BOM:   (Linux/OS X)  tail -c +4   DockerrunFile     DockerrunFileNew  Replace  DockerrunFile  with the path to your Dockerrun file, and  DockerrunFileNew  with a new name for the Dockerrun file without the BOM.     Alternatively, you can use the  dos2unix file converter  to remove the BOM from your Dockerrun files. Dos2unix is available for Windows, Linux and Mac OS.  To remove the BOM using dos2unix:    At the command line, type the following:   dos2unix --remove-bom -n   DockerrunFile   DockerrunFileNew  Replace  DockerrunFile  with the path to your Dockerrun file, and  DockerrunFileNew  with a new name for the Dockerrun file without the BOM.", 
            "title": "Byte Order Marks (BOM) in Dockerrun file"
        }, 
        {
            "location": "/troubleshooting/commonissues/#aws-error-the-key-pair-does-not-exist-code-validationerror-with-l0-setup", 
            "text": "This occurs when you pass a non-existent EC2 keypair to l0-setup. To fix this, follow the instructions for  creating an EC2 Key Pair .   After you've created a new EC2 Key Pair, run the following command: \n   l0-setup plan   prefix   -var key_pair = keypair", 
            "title": "\"AWS Error: the key pair '' does not exist (code 'ValidationError')\" with l0-setup"
        }, 
        {
            "location": "/troubleshooting/ssh/", 
            "text": "Secure Shell (SSH)\n#\n\n\nYou can use Secure Shell (SSH) to access your Layer0 environment(s).\n\n\nBy default, Layer0 Setup asks for an EC2 key pair when creating a new Layer0. This key pair is associated with all machines that host your Layer0 Services. This means you can use SSH to log into the underlying Docker host to perform tasks such as troubleshooting failing containers or viewing logs. For information about creating an EC2 key pair, see \nInstall and Configure Layer0\n.\n\n\n\n\nWarning\n\n\nThis section is recommended for development debugging only.\nIt is \nnot\n recommended for production environments.\n\n\n\n\nTo SSH into a Service\n#\n\n\n\n\nIn a console window, add port 2222:22/tcp to your Service's load balancer:\n\n\n\n\nl0 loadbalancer addport \nname\n 2222:22/tcp\n\n\n\n\n\n  \nSSH into your Service by supplying the load balancer url and key pair file name.\n\n\n\n\n\nssh -i \nkey pair path and file name\n ec2-user@\nload balancer url\n -p 2222\n\n\n\n\n\n  \nIf required, Use Docker to access a specific container with Bash.\n\n\n\n\n\ndocker exec -it \ncontainer id\n /bin/bash\n\n\n\n\nRemarks\n#\n\n\nYou can get the load balancer url from the Load Balancers section of your Layer0 AWS console.\n\n\nUse the \nloadbalancer dropport\n subcommand to remove a port configuration from an existing Layer0 load balancer.\n\n\nYou \ncannot\n change the key pair after a Layer0 has been created. If you lose your key pair or need to generate a new one, you will need to create a new Layer0.\n\n\nIf your Service is behind a private load balancer, or none at all, you can either re-create your Service behind a public load balancer, use an existing public load balancer as a \"jump\" point, or create a new Layer0 Service behind a public load balancer to serve as a \"jump\" point.", 
            "title": "Secure Shell (SSH)"
        }, 
        {
            "location": "/troubleshooting/ssh/#secure-shell-ssh", 
            "text": "You can use Secure Shell (SSH) to access your Layer0 environment(s).  By default, Layer0 Setup asks for an EC2 key pair when creating a new Layer0. This key pair is associated with all machines that host your Layer0 Services. This means you can use SSH to log into the underlying Docker host to perform tasks such as troubleshooting failing containers or viewing logs. For information about creating an EC2 key pair, see  Install and Configure Layer0 .   Warning  This section is recommended for development debugging only.\nIt is  not  recommended for production environments.", 
            "title": "Secure Shell (SSH)"
        }, 
        {
            "location": "/troubleshooting/ssh/#to-ssh-into-a-service", 
            "text": "In a console window, add port 2222:22/tcp to your Service's load balancer:   l0 loadbalancer addport  name  2222:22/tcp  \n   SSH into your Service by supplying the load balancer url and key pair file name.   ssh -i  key pair path and file name  ec2-user@ load balancer url  -p 2222  \n   If required, Use Docker to access a specific container with Bash.   docker exec -it  container id  /bin/bash", 
            "title": "To SSH into a Service"
        }, 
        {
            "location": "/troubleshooting/ssh/#remarks", 
            "text": "You can get the load balancer url from the Load Balancers section of your Layer0 AWS console.  Use the  loadbalancer dropport  subcommand to remove a port configuration from an existing Layer0 load balancer.  You  cannot  change the key pair after a Layer0 has been created. If you lose your key pair or need to generate a new one, you will need to create a new Layer0.  If your Service is behind a private load balancer, or none at all, you can either re-create your Service behind a public load balancer, use an existing public load balancer as a \"jump\" point, or create a new Layer0 Service behind a public load balancer to serve as a \"jump\" point.", 
            "title": "Remarks"
        }
    ]
}